\chapter{Multivariate regression}
\minitoc
\begin{kwd}

\end{kwd}

\section{Gaussian vectors}

\begin{shaded}
\begin{definition}
\label{def:gauss:vec}
A random variable $X\in\rset^d$ is a Gaussian vector if and only if, for all $a\in\rset^d$, the random variable $\langle a\eqsp;\eqsp X\rangle$ is a Gaussian random variable.
\end{definition}
\end{shaded}
For all random variable $X\in\rset^d$,  $X\sim \mathcal{N}(\mu,\Sigma)$ means that $X$ is a Gaussian vector with mean $\bE[X] = \mu\in\rset^n$ and covariance matrix $\bV[X] = \Sigma\in\rset^{n\times n}$. The characteristic function of $X$ is given (see exercises), for all $t\in\rset^n$, by
\[
\bE[\rme^{i\langle t\eqsp;\eqsp X\rangle}] = \rme^{i\langle t\eqsp;\eqsp \mu\rangle - t^\top\Sigma t /2}\eqsp.
\]
Therefore, the law of a Gaussian vector is uniquely defined by its mean vector and its covariance matrix.
If the covariance matrix $\Sigma$ is nonsingular, then the law of $X$ has a probability density with respect to the Lebesgue measure on $\rset^n$ given by : 
\[
x\mapsto \mathrm{det}(2\pi \Sigma)^{-1/2} \exp\left\{-(x-\mu)^\top\Sigma^{-1}(x-\mu)/2\right\}\eqsp,
\]
where $\mu = \bE[X]$.

\begin{shaded}
\begin{proposition}
\label{prop:gauss:vec:decor}
Let $X\in\rset^d$ be a Gaussian vector. Let $\{i_1,\ldots,i_p\}$ be a subset of $\{1,\ldots,d\}$, $p\geqslant 1$. If for all $1\leqslant k\neq j \leqslant p$, $\mathrm{Cov}(X_{i_k},X_{i_j}) = 0$, then $(X_{i_1},\ldots,X_{i_p})$ are independent.
\end{proposition}
\end{shaded}
\begin{proof}
The random vector $(X_{i_1},\ldots,X_{i_p})^\top$ is a Gaussian vector with mean $(\bE[X_{i_1}],\ldots,\bE[X_{i_p}])^\top$ and diagonal covariance matrix $\mathrm{diag}(\bV[X_{i_1}],\ldots,\bV[X_{i_p}])$. Consider $(\xi_{i_1},\ldots,\xi_{i_p})$ i.i.d. random variables with distribution $\mathcal{N}(0,1)$ and define, for all $1\leqslant j \leqslant p$,
\[
Z_{i_j} = \bE[X_{i_j}]  + \sqrt{\bV[X_{i_j}]}\xi_{i_j}\eqsp.
\]
Then, the random vector $(Z_{i_1},\ldots,Z_{i_p})^\top$ is a Gaussian vector with the same mean and the same covariance matrix as $(X_{i_1},\ldots,X_{i_p})^\top$. The two vectors have therefore the same characteristic function and the same law and $(X_{i_1},\ldots,X_{i_p})$  are independent as  $(\xi_{i_1},\ldots,\xi_{i_p})$ are independent.
\end{proof}

\begin{shaded}
\begin{theorem}[Cochran]
\label{th:cochran}
Let $X\sim \mathcal{N}(0,I_d)$ be a Gaussian vector in $\rset^d$, $F$ be a vector subspace of $\rset^d$ and $F^{\perp}$ its orthogonal. Denote by $\pi_F(X)$ (resp. $\pi_{F^{\perp}}(X)$) the orthogonal projection of $X$ on $F$ (resp. on $F^{\perp}$). Then,  $\pi_F(X)$ and $\pi_{F^{\perp}}(X)$ are independent, $\|\pi_F(X)\|_2^2\sim\chi^2(p)$ and $\|\pi_{F^{\perp}}(X)\|_2^2 \sim \chi^2(d-p)$, where $p$ is the dimension of $F$.
\end{theorem}
\end{shaded}
\begin{proof}
Let $(u_1,\ldots,u_d)$ be an orthonormal basis of $\rset^d$ where $(u_1,\ldots,u_p)$ is an orthonormal basis of $F$ and $(u_{p+1},\ldots,u_d)$ and orthonormal basis of $F^{\perp}$. Consider the matrix $U\in\rset^{d\times d}$ such that for all $1\leqslant i\leqslant d$, the $i$-th column of $U$ is $u_i$ and $U_{(p)}$ (reps. $U_{(d-p)}^{\perp}$) the matrix made of the first $p$ (resp. last $d-p$) columns of $U$. Note that
\[
\pi_F(X) = \sum_{i=1}^p \langle X\eqsp;\eqsp u_i\rangle u_i\eqsp,
\]
which can be written $\pi_F(X) = U_{(p)}U_{(p)}^\top X$. Similarly, $\pi_{F^{\perp}}(X) = U^{\perp}_{(d-p)}(U^{\perp}_{(d-p)})^\top X$
Therefore,
\[
\begin{pmatrix}\pi_F(X)\\ \pi_{F^{\perp}}(X)\end{pmatrix}  = \begin{pmatrix} U_{(p)}U_{(p)}^\top \\ U^{\perp}_{(d-p)}(U^{\perp}_{(d-p)})^\top\end{pmatrix}X
\]
is a centered Gaussian vector with covariance matrix given by
\[
 \begin{pmatrix} U_{(p)}U_{(p)}^\top &  0 \\ 0 & U^{\perp}_{(d-p)}(U^{\perp}_{(d-p)})^\top\end{pmatrix}\eqsp.
\]
By Proposition~\ref{prop:gauss:vec:decor}, $\pi_F(X)$ and $\pi_{F^{\perp}}(X)$ are independent. On the other hand,
\[
\|\pi_F(X)\|_2^2 = \sum_{i=1}^p \langle X\eqsp;\eqsp u_i\rangle^2 \quad\mathrm{and} \quad \|\pi_{F^{\perp}}(X)\|_2^2 = \sum_{i=p+1}^d \langle X\eqsp;\eqsp u_i\rangle^2\eqsp. 
\]
The random vector $(\langle X\eqsp;\eqsp u_i\rangle)_{1\leqslant i \leqslant d}$ is given by $U^TX$: it is a Gaussian random vector with mean $0$ and covariance matrix $I_d$. The random variables $(\langle X\eqsp;\eqsp u_i\rangle)_{1\leqslant i \leqslant d}$ are therefore i.i.d. with distribution $\mathcal{N}(0,1)$, which concludes the proof.
\end{proof}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%ù%

\section{Full rank multivariate regression}
\label{sec:full:rank:reg}

\subsection{Preliminaries}
In a supervised learning framework, a set $\{(X_i,Y_i)\}_{1\leqslant i \leqslant n}$  of input data (also referred to as {\em features}) $X_i\in\xset$ and output data $Y_i\in\yset$ (also referred to as {\em observations}), $1\leqslant i \leqslant n$, is available, where $\xset$ is a general feture space and $\yset$ is a general observation space. For instance, in a supervised classification framework, the problem is to learn wether an individual from a given state space $\xset$ belongs to some class in $\yset = \{1,\ldots,M\}$. The state space $\xset$ is usually a subset of $\rset^d$ and an element of $\xset$ contains all the features ued to predict the associated observation. In a regression framework, the observation set $\yset$ is usually a subset of $\rset^m$.


Our aim is to introduce a regression function using the training dataset $\{(X_i,Y_i)\}_{1\leqslant i \leqslant n}$ employed to predict the observations associated with new features in a test dataset. In these lecture notes, we focus on empirical risk minimization. We consider a parameter set $\paramset$ and a family of regression functions $\{f_\param\}_{\param\in\paramset}$ where for all $\param\in\paramset$, $f_\param: \xset\to \yset$. Considering first that $\yset = \rset$ and  $\xset = \rset^d$  we focus on solving the following optimization problem:
$$
\widehat \param_n\in  \argmin_{\param\in\paramset}  \frac{1}{n}\sum_{i=1}^n\left(Y_i-f_\param(X_i)\right)^2\eqsp.
$$
The components of the vector $\widehat \param_n$ are often referred to as the {\em weights} or the {\em regression coefficients}. Each component $\widehat \param_n(j)$, $1\leqslant j \leqslant d$, specifies the expected change in the output when the input $X(j)$ is changed by one unit.

\subsection{Least squares estimator}
In a linear regression setting, we assume that $\paramset = \rset^d$ and that  for all $\param\in\paramset$, $f_\param: x \mapsto \param^\top x$.  Let $Y\in\rset^d$  be the random (column) vector such that  for all $1\leqslant i \leqslant n$, the $i$-th component of $Y$ is $Y_i$  and $X\in\rset^{n\times d}$ the matrix with line $i$ equal to $X^\top_i$. 
%It is assumed that for all $1\leqslant i \leqslant n$, $Y_i = X^T_i \beta_{\star} + \varepsilon_i$ where the $(\varepsilon_i)_{1\leqslant i\leqslant n}$ are i.i.d. random variables in $\rset^d$, $X_i\in\rset^d$ and $\beta_{\star}$ is an unknown vector in $\rset^d$. Let $Y\in\rset^d$ (resp. $\varepsilon\in\rset^d$)  be the random vector such that  for all $1\leqslant i \leqslant n$, the $i$-th component of $Y$ (resp. $\varepsilon$) is $Y_i$ (resp. $\varepsilon_i$) and $X\in\rset^{n\times d}$ the matrix with line $i$ equal to $X^T_i$. The model is then written
%\[
%Y = X \beta_{\star} + \varepsilon\eqsp.
%\]
%In this section, it is assumed that $\bE[\varepsilon] = 0$ and $\bE[\varepsilon \varepsilon^T] = \sigma_{\star}^2 I_n$ and that the matrix $X$ has full rank, i.e. the columns of $X$ are linearly independent. 
The least squares estimate is defined as a solution to
\[
\widehat \param_n\in  \argmin_{\param\in\rset^d}  \|Y - X\param\|_2^2\eqsp.
\]
In a {\em well-specified setting}, we assumed that  for all $1\leqslant i \leqslant n$, $Y_i = X^\top_i \param_{\star} + \varepsilon_i$ for some unknown $\param_\star\in\rset^d$ where the $(\varepsilon_i)_{1\leqslant i\leqslant n}$ are i.i.d. random variables in $\rset$. Let  $\varepsilon\in\rset^n$  be the random vector such that  for all $1\leqslant i \leqslant n$, the $i$-th component of $\varepsilon$  is $\varepsilon_i$. The model is then written
\[
Y = X \param_{\star} + \varepsilon\eqsp.
\]

\begin{remark}
If the matrix $X$ has full rank, i.e. its columns are linearly independent, then $X^\top X$ is positive definite since for all $u\in\rset^d$, $u^\top X^\top X u = \|Xu\|_2^2$ and therefore  $u^\top X^\top X u \geqslant 0$ and $u^\top X^\top X u = 0$ if and only if $Xu = 0$ i.e. if $u=0$.
\end{remark}

\begin{shaded}
\begin{proposition}
\label{prop:least:squares:full:rank}
If the matrix $X$ has full rank, then, $\widehat \param_n = (X^\top X)^{-1}X^\top Y$. If for all $1\leqslant i \leqslant n$, $Y_i = X^\top_i \param_{\star} + \varepsilon_i$ for some unknown $\param_\star\in\rset^d$ where the $(\varepsilon_i)_{1\leqslant i\leqslant n}$ are i.i.d. centered random variables in $\rset$ with variance $\sigma_\star^2$, $\widehat \param_n$  is an unbiased estimator of $\param_\star$ and it satisfies $\mathbb{V}[\widehat \param_n] = \sigma_\star^2 (X^\top X)^{-1}$.
\end{proposition}
\end{shaded}

\begin{proof}
For all $\param\in\rset^d$,
\[
 \|Y - X\param\|_2^2 = \|Y\|_2^2 + \param^\top X^\top X\param + 2Y^\top X\param\eqsp.
\]
The function $\ell:\param \mapsto\|Y\|_2^2 + \param^\top X^\top X\param + 2Y^\top X\param$ is convex and for all $\param\in\rset^d$,
\[
\nabla \ell (\param) = 2X^\top X\param + 2 X^\top Y\eqsp.
\]
As the matrix  $X$ has full rank, $X^\top X$ is nonsingular and $\nabla \ell (\param) =0$ has a unique solution given by
\[
\widehat \param_n = (X^\top X)^{-1}X^\top Y\eqsp.
\]
First, note that $\widehat \param_n$ is unbiased as
\[
\bE[\widehat \param_n] = (X^\top X)^{-1}X^\top \bE[Y] = (X^\top X)^{-1}X^\top X\param_{\star} = \param_\star\eqsp.
\]
In addition,
\[
\bV[\widehat \param_n] = (X^\top X)^{-1}X^\top\bV[Y]X(X^\top X)^{-1}= \sigma_\star^2(X^\top X)^{-1}X^\top X(X^\top X)^{-1} = \sigma_\star^2(X^\top X)^{-1}\eqsp.
\]
\end{proof}

\begin{remark}
If we assume that  for all $1\leqslant i \leqslant n$, $Y_i = X^\top_i \param_{\star} + \varepsilon_i$ for some unknown $\param_\star\in\rset^d$ where the $(\varepsilon_i)_{1\leqslant i\leqslant n}$ are i.i.d. random variables in $\rset$ with distribution $\mathcal{N}(0,\sigma_*^2)$, $\widehat \param_n$ is the maximum likelihood estimator of $\param_\star$. The loglikelihood of the observations writes, for all $\param\in\paramset$,
$$
\log p_\theta(Y_{1:n}) = \sum_{i=1}^{n}\log p_\theta(Y_{i}) =  \sum_{i=1}^{n}\left\{- \frac{1}{2}\log(2\pi \sigma_\star^2) - \frac{1}{2}\left(Y_i - \param^\top X_i\right)^2\right\}\eqsp.
$$
Therefore, maximizing $\param\mapsto \log p_\theta(Y_{1:n})$ amounts to minimizing $\param \mapsto \sum_{i=1}^{n}(Y_i - \param^\top X_i)^2 = \|Y - X\param\|_2^2$.
\end{remark}

\begin{remark}
\label{reg:linear:projX}
The matrix $X(X^\top X)^{-1}X^\top \in\rset^{n\times n}$ is the matrix of the orthogonal projection onto $\mathrm{Range}(X)$, i.e., the vector space generated by the column vectors of $X$. First, let $v \in \mathrm{Range}(X)$, then, there exists $u\in\rset^d$ such that $v = Xu$ and $X(X^\top X)^{-1}X^\top v = X(X^\top X)^{-1}X^\top Xu = Xu = v$. Therefore, for all $v \in \mathrm{Range}(X)$, $X(X^\top X)^{-1}X^\top v=v$. In addition, for all $v\in \mathrm{Range}(X)^\perp$, $X^\top v = 0$ so that $X(X^\top X)^{-1}X^\top v = 0$. 
\end{remark}

\begin{remark}
\label{reg:linear:projX:d1}
The projected value of $Y$ is 
$$
\widehat Y = X \widehat \param_n = X(X^\top X)^{-1}X^\top Y\eqsp.
$$
In the special case where $d = 1$, $X\in\rset^n$ and $\widehat Y = \{X^\top Y/(X^\top X)\}X = \{\langle X ; Y\rangle / \langle X ; X \rangle\}X $.
\end{remark}

\begin{shaded}
\begin{proposition}
\label{prop:least:squares:full:rank:variance}
If $Y = X\theta_\star + \varepsilon$, where $\varepsilon \sim \mathcal{N}(0,\sigma_\star^2\Id_n)$, the random variable
\[
\widehat \sigma^2_n =\frac{\|Y - X\widehat \param_n \|_2^2}{n-d}
\]
is an unbiased estimator of $\sigma_\star$. In addition, $(n-d)\widehat \sigma^2_n/\sigma_\star^2\sim\chi^2(n-d)$, $\widehat \param_n \sim \mathcal{N}(\param_{\star},\sigma_\star^2(X^\top X)^{-1})$ and $\widehat \param_n$ and $\widehat \sigma^2_n$ are independent.
\end{proposition}
\end{shaded}

\begin{proof}
By definition of $\widehat \param_n$,
\[
\widehat \sigma^2_n =\frac{\|Y - X\widehat \param_n \|_2^2}{n-d} = \frac{\|Y - X(X^\top X)^{-1}X^\top Y\|_2^2}{n-d} = \frac{\|(I_n - X(X^\top X)^{-1}X^\top )Y\|_2^2}{n-d}
\]
By Remark~\ref{reg:linear:projX}, the matrix of the orthogonal projection on $\mathrm{Range}(X)$ is $X(X^\top X)^{-1}X^\top$ and therefore $(I_n - X(X^\top X)^{-1}X^\top$ is the matrix  of the orthogonal projection on $\mathrm{Range}(X)^{\perp}$. Then,
\[
(I_n - X(X^\top X)^{-1}X^\top)Y = (I_n - X(X^\top X)^{-1}X^\top)(X\param_\star + \varepsilon) = (I_n - X(X^\top X)^{-1}X^\top)\varepsilon\eqsp.
\]
By Theorem~\ref{th:cochran}, $\|\sigma_\star^{-1}(I_n - X(X^\top X)^{-1}X^\top)\varepsilon\|_2^2$ has a $\chi^2$ distribution with $n-d$ degrees of freedom which yields
\[
\bE[\|(I_n - X(X^\top X)^{-1}X^\top)Y\|_2^2] = \sigma_\star^2(n-d)
\]
and $\bE[\widehat \sigma^2_n ] = \sigma_\star^2$. By Proposition~\ref{prop:least:squares:full:rank}, $\bE[\widehat \param_n] = \param_\star$ and $\bV[\widehat \param_n] = \sigma_\star^2(X^\top X)^{-1}$ and $\widehat \param_n$ is a Gaussian vector as an  affine transformation of a Gaussian vector. Note that $(n-d)\widehat \sigma^2_n = \|(I_n - X(X^\top X)^{-1}X^\top)\varepsilon\|_2^2$ and $\widehat \param_n = (X^\top X)^{-1}X^\top X\param_{\star} + (X^\top X)^{-1}X^\top\varepsilon$ and that $(X^\top X)^{-1}X^\top\varepsilon$ and $(I_n - X(X^\top X)^{-1}X^\top)\varepsilon$ are not correlated as 
\[
\bE[(I_n - X(X^\top X)^{-1}X^\top)\varepsilon \varepsilon^\top X(X^\top X)^{-1}] = \sigma_\star^2\bE[(I_n - X(X^\top X)^{-1}X^\top)X(X^\top X)^{-1}] = 0\eqsp.
\]
The independence follows from Proposition~\ref{prop:gauss:vec:decor}.
\end{proof}

\subsection{Computational issues}
Eventhough it is possible to compute  the inverse of $X^\top X$ in a full rank setting, this matrix can be ill conditioned which may lead to numerical instability. 
\begin{itemize}
\item In Scikit-learn, the fit function of \url{https://scikit-learn.org/stable/modules/generated/sklearn.linear\_model.LinearRegression.html} uses a SVD-based solver. By Proposition~\ref{prop:acp:svd}, if $X$ has  rank $r\geqslant 1$, there exist $\sigma_1\geqslant \ldots \geqslant \sigma_r>0$ such that
\[
X = \sum_{k=1}^r \sigma_k u_k v^\top_k\eqsp,
\]
where $\{u_1,\ldots,u_r\}\in (\rset^n)^r$ and $\{v_1,\ldots,v_r\}\in (\rset^d)^r$ are two orthonormal families. The vectors $\{\sigma_1,\ldots,\sigma_r\}$ are called singular values of $A$ and $\{u_1,\ldots,u_r\}$ (resp. $\{v_1,\ldots,v_r\}$) are the left-singular (resp. right-singular) vectors of $X$. If $U$ denotes the $\rset^{n\times r}$ matrix with columns given by $\{u_1,\ldots,u_r\}$ and $V$ denotes the $\rset^{d \times r}$ matrix with columns given by $\{v_1,\ldots,v_r\}$, then the singular value decomposition of $A$ may also be written as
\[
X = UD_rV^\top\eqsp,
\]
where $D_r = \mathrm{diag}(\sigma_1,\ldots,\sigma_r)$. Therefore 
$$
X^\top X = VD^2_rV^\top\quad\mathrm{and}\quad (X^\top X)^{-1} =  VD^{-2}_rV^\top\eqsp.
$$
In this case, it is enough to compute $V$ and $D$ to compute $(X^\top X)^{-1}$.
\item Using QR decomposition, we know that there exist an orthogonal matrix $Q\in\rset^{n\times d}$, $Q^\top Q = I_d$, and an upper triangular matrix $R\in\rset^{d\times d}$ such that $X = QR$. Then,
$$
X^\top X \widehat\param_n = X^\top Y \Leftrightarrow R\widehat \param_n = Q^\top Y\eqsp.
$$
As the matrix $R$ is upper triangular, the last equation can be solved using backsubstitution using for instance. The estimator $\widehat\param_n$ can then de computed by i) computing the QR factorization of $X$, ii) computing $Q^\top Y$ and iii) solving $R\widehat \param_n = Q^\top Y$.
\end{itemize}
\section{Risk analysis of the full-rank multivariate regression}
\label{sec:risk:fullrank}
In our fixed-design setting, where the matrix $X$ is deterministic, our aim is to minimize the fixed design risk:
$$
\mathsf{R}(\param) = \frac{1}{n}\bE\left[\left\|Y - X\param\right\|_2^2\right]\eqsp.
$$
If for all $1\leqslant i \leqslant n$, $Y_i = X^\top_i \param_{\star} + \varepsilon_i$ for some unknown $\param_\star\in\rset^d$ where the $(\varepsilon_i)_{1\leqslant i\leqslant n}$ are i.i.d. centered random variables in $\rset$ with variance $\sigma_\star^2$, note that
$$
\mathsf{R}(\param_\star) = \frac{1}{n}\bE\left[\left\|Y - X\param_\star\right\|_2^2\right]= \frac{1}{n}\bE\left[\left\|\varepsilon\right\|_2^2\right] = \sigma_\star^2\eqsp.
$$
Therefore, for all $\param\in\paramset$,
\begin{align*}
\mathsf{R}(\param) - \mathsf{R}(\param_\star) &=    \frac{1}{n}\bE\left[\left\|X\param_\star + \varepsilon - X\param\right\|_2^2\right] - \sigma_\star^2\eqsp,\\
&=  \frac{1}{n}\bE\left[\left\|X(\param_\star-\param)\right\|_2^2 + \left\|\varepsilon\right\|_2^2 + 2 \left(\param_\star-\param\right)^\top X^\top \varepsilon\right] - \sigma_\star^2\eqsp,\\
&= \left(\param_\star-\param\right)^\top \left(\frac{1}{n}X^\top X\right) \left(\param_\star-\param\right)\eqsp,
\end{align*}
since $\mathbb{E}[\varepsilon] = 0$. On the other hand, a standard bias-variance decomposition yields
\begin{align*}
\bE\left[\mathsf{R}(\widehat \param_n) - \mathsf{R}(\param_\star)\right] &=  \bE\left[ \left(\param_\star-\widehat \param_n\right)^\top \left(\frac{1}{n}X^\top X\right) \left(\param_\star-\widehat \param_n\right)\right]\eqsp,\\
&=  \bE\left[ \left(\param_\star-\bE\left[\widehat \param_n\right] + \bE\left[\widehat \param_n\right] - \widehat \param_n\right)^\top \left(\frac{1}{n}X^\top X\right) \left(\param_\star-\bE\left[\widehat \param_n\right] + \bE\left[\widehat \param_n\right] - \widehat \param_n\right)\right]\eqsp,\\
&=\left(\param_\star-\bE\left[\widehat \param_n\right]\right)^\top \left(\frac{1}{n}X^\top X\right) \left(\param_\star-\bE\left[\widehat \param_n\right] \right) + \bE\left[ \left(\bE\left[\widehat \param_n\right] - \widehat \param_n\right)^\top \left(\frac{1}{n}X^\top X\right) \left(\bE\left[\widehat \param_n\right] - \widehat \param_n\right)\right]\eqsp.
\end{align*}


\begin{proposition}
If for all $1\leqslant i \leqslant n$, $Y_i = X^\top_i \param_{\star} + \varepsilon_i$ for some unknown $\param_\star\in\rset^d$ where the $(\varepsilon_i)_{1\leqslant i\leqslant n}$ are i.i.d. centered random variables in $\rset$ with variance $\sigma_\star^2$, 
$$
\bE\left[\mathsf{R}(\widehat \param_n) - \mathsf{R}(\param_\star)\right] = \sigma_\star^2 \frac{d}{n}\eqsp.
$$
\end{proposition}

\begin{proof}
By Proposition~\ref{prop:least:squares:full:rank}, $\bE[\widehat \param_n] = \param_\star$ so that 
$$
\bE\left[\mathsf{R}(\widehat \param_n) - \mathsf{R}(\param_\star)\right] = \bE\left[ \left(\bE\left[\widehat \param_n\right] - \widehat \param_n\right)^\top \left(\frac{1}{n}X^\top X\right) \left(\bE\left[\widehat \param_n\right] - \widehat \param_n\right)\right]\eqsp.
$$
In addition, by Proposition~\ref{prop:least:squares:full:rank}, $\bV[\widehat \param_n] = \sigma_\star^2(X^\top X)^{-1}$, hence
$$
\bE\left[\mathsf{R}(\widehat \param_n) - \mathsf{R}(\param_\star)\right] = \frac{\sigma_\star^2}{n}\bE\left[ \left(\bE\left[\widehat \param_n\right] - \widehat \param_n\right)^\top \bV[\widehat \param_n]^{-1} \left(\bE\left[\widehat \param_n\right] - \widehat \param_n\right)\right]\eqsp.
$$
By Lemma~\ref{ref:expectation:quadratic:trasform},
$$
\bE\left[\mathsf{R}(\widehat \param_n) - \mathsf{R}(\param_\star)\right] = \frac{\sigma_\star^2}{n}\mathrm{Trace}(\bV[\widehat \param_n]^{-1}\bV[\widehat \param_n]) = \sigma_\star^2 \frac{d}{n}\eqsp.
$$
\end{proof}

\section{Confidence intervals and tests}
\subsection*{Student's t-statistics}
\begin{shaded}
\begin{proposition}
\label{prop:betaj}
Assume that  for all $1\leqslant i \leqslant n$, $Y_i = X^\top_i \param_{\star} + \varepsilon_i$ for some unknown $\param_\star\in\rset^d$ where the $(\varepsilon_i)_{1\leqslant i\leqslant n}$ are i.i.d. with distribution $\mathcal{N}(0,\sigma_\star^2)$. For all $1\leqslant j \leqslant d$,
\[
\frac{\widehat \param_{n,j} -\param_{\star,j}}{\widehat\sigma_{n}\sqrt{(X^TX)_{j,j}^{-1}}} \sim \mathcal{S}(n-d)\eqsp,
\]
where $\mathcal{S}(n-d)$ is the Student's t-distribution with $n-p$ degrees of freedom, i.e. the law of $X/\sqrt{Y/(n-d)}$ where $X\sim\mathcal{N}(0,1)$ is independent of $Y\sim\chi^2(n-d)$.
\end{proposition}
\end{shaded}
\begin{proof}
By definition, for all $1\leqslant j \leqslant d$,  
\[
\frac{\widehat \param_{n,j} -\param_{\star,j}}{\widehat\sigma_{n}\sqrt{(X^\top X)_{j,j}^{-1}}} = \frac{\sigma_\star^{-1}(\widehat \param_{n,j} -\param_{\star,j})}{\sigma_\star^{-1}\widehat\sigma_{n}\sqrt{(X^\top X)_{j,j}^{-1}}} = \frac{e^\top_j(\sigma_\star^{-1}(\widehat \param_{n} -\param_{\star}))}{\sigma_\star^{-1}\widehat\sigma_{n}\sqrt{(X^\top X)_{j,j}^{-1}}}\eqsp.
\]
Note that $\sigma_\star^{-1}(\widehat \param_{n} -\param_{\star}) \sim \mathcal{N}(0,(X^\top X)^{-1})$ so that $e^\top_j(\sigma_\star^{-1}(\widehat \param_{n} -\param_{\star}))\sim \mathcal{N}(0,e^\top_j(X^\top X)^{-1}e_j)$ and 
\[
\frac{e^\top_j(\sigma_\star^{-1}(\widehat \param_{n} -\param_{\star}))}{\sqrt{(X^\top X)_{j,j}^{-1}}}\sim\mathcal{N}(0,1)\eqsp.
\]
In addition,
\[
\sigma_\star^{-1}\widehat\sigma_{n} = \sqrt{\sigma_\star^{-2}\widehat\sigma^2_{n}} = \sqrt{\|\sigma_\star^{-1}(I_n - X(X^\top X)^{-1}X^\top)\varepsilon\|_2^2/(n-d)}\eqsp,
\]
where $\sigma_\star^{-2}\widehat\sigma^2_{n} = \|\sigma_\star^{-1}(I_n - X(X^\top X)^{-1}X^\top)\varepsilon\|_2^2 \sim \chi^2(n-d)$. The proof is concluded by noting that $\widehat \param_n$ and  $\widehat\sigma^2_{n}$ are independent.
\end{proof}
By Proposition~\ref{prop:betaj}, for $\alpha\in(0,1)$, if $s_{1-\alpha/2}^{n-d}$ denotes the quantile of order $1-\alpha/2$ of the law $\mathcal{S}(n-d)$, then 
\[
\bP\left(\left|\frac{\widehat \param_{n,j} -\param_{\star,j}}{\widehat\sigma_{n}\sqrt{(X^TX)_{j,j}^{-1}}}\right| \leqslant s_{1-\alpha/2}^{n-d}\right) = 1-\alpha\eqsp.
\]
Therefore, 
\[
I^{n-p}_{n,j}(\param_{\star}) = \left[\widehat \param_{n,j} -\widehat\sigma_{n}s_{1-\alpha/2}^{n-d}\sqrt{(X^\top X)_{j,j}^{-1}}\eqsp;\eqsp \widehat \param_{n,j} + \widehat\sigma_{n}s_{1-\alpha/2}^{n-d}\sqrt{(X^\top X)_{j,j}^{-1}}\right]
\]
 is a confidence interval for $\param_{\star,j}$ with confidence level $1-\alpha$. The result of Proposition~\ref{prop:betaj} may also be used to perform the test 
\[
\mathrm{H}_0:\eqsp \param_{\star,j} = 0 \quad\mathrm{vs} \quad \mathrm{H}_1:\eqsp \param_{\star,j} \neq 0\eqsp.
\]
Under $\mathrm{H}_0$, the random variable $T_{n,j}$ defined by 
\[
T_{n,j} = \frac{\widehat \param_{n,j}}{\widehat\sigma_{n}\sqrt{(X^\top X)_{j,j}^{-1}}} 
\] 
does not depend  on $\param_{\star}$ neither on $\sigma_{\star}$ and is distributed as a Student $\mathcal{S}(n-d)$ random variable. A statistical test with statistical signifiance $1-\alpha$ to decide wether $\param_{\star}\neq 0$ is $T_{n,j}<s_{1-\alpha/2}^{n-d}$. 
\subsection*{Fisher statistics}
\begin{shaded}
\begin{proposition}
\label{prop:Lbeta}
Assume that for all $1\leqslant i \leqslant n$, $Y_i = X^\top_i \param_{\star} + \varepsilon_i$ for some unknown $\param_\star\in\rset^d$ where the $(\varepsilon_i)_{1\leqslant i\leqslant n}$ are i.i.d. with distribution $\mathcal{N}(0,\sigma_\star^2)$. Let $L$ be a $\rset^{q\times d}$ matrix with rank $q\leqslant d$. Then,
\[
\frac{(\widehat \param_{n} -\param_{\star})^\top L^\top(L(X^\top X)^{-1}L^\top)^{-1}L(\widehat \param_{n} -\param_{\star})}{q\widehat\sigma^2_{n}} \sim \mathcal{F}(q,n-d)\eqsp,
\]
where $\mathcal{F}(q,n-d)$ is the Fisher distribution with $q$ and $n-d$ degrees of freedom, i.e. the law of $(X/q)/(Y/(n-d))$ where $X\sim\chi^2(q)$ is independent of $Y\sim\chi^2(n-d)$.
\end{proposition}
\end{shaded}
\begin{proof}
Note that $\mathrm{rank}(L(X^\top X)^{-1}L^\top) = \mathrm{rank}(LL^\top)  = q$. The matrix $L(X^\top X)^{-1}L^\top$ is therefore positive definite. There exists a diagonal matrix $D\in\rset^{q\times q}$ with positive diagonal terms and an orthogonal matrix $Q\in\rset^{q\times q}$ such that $L(X^\top X)^{-1}L^\top = QDQ^{-1}$. The matrix $(L(X^\top X)^{-1}L^\top)^{-1/2}$ may be defined as $(L(X^\top X)^{-1}L^\top)^{-1/2} = QD^{-1/2}Q^{-1}$. It is then enough to note that $(L(X^\top X)^{-1}L^\top)^{-1/2}L(\widehat \param_{n} -\param_{\star})/\sigma_{\star}\sim \mathcal{N}(0,I_q)$. Therefore,
\[
\sigma^{-2}_{\star}\|(L(X^\top X)^{-1}L^\top)^{-1/2}L(\widehat \param_{n} -\param_{\star})\|^2 = (\widehat \param_{n} -\param_{\star})^\top L^\top(L(X^\top X)^{-1}L^\top)^{-1}L(\widehat \param_{n} -\param_{\star})/\sigma^2_{\star} \sim \chi^2(q)\eqsp.
\]
On the other hand, by Proposition~\ref{prop:least:squares:full:rank}, 
\[
(n-d)\sigma^{-2}_{\star}\widehat\sigma^2_{n} \sim \chi^2(n-d)\eqsp.
\]
The proof is concluded by noting that $\widehat \param_n$ and  $\widehat\sigma^2_{n}$ are independent.
\end{proof}
By Proposition~\ref{prop:Lbeta}, for $\alpha\in(0,1)$, if $f_{1-\alpha}^{q,n-d}$ denotes the quantile of order $1-\alpha$ of the law $\mathcal{F}(q,n-p)$, then 
\[
\bP\left(\param_{\star}\in \left\{\param\in\rset^d\eqsp;\eqsp (\widehat \param_{n} -\param)^\top L^\top(L(X^\top X)^{-1}L^\top)^{-1}L(\widehat \param_{n} -\param)\leqslant q\widehat\sigma^2_{n}f_{1-\alpha}^{q,n-d}\right\}\right) = 1-\alpha\eqsp.
\]
Therefore, 
\[
I^{q,n-d}_{n}(\param_{\star}) = \left\{\param\in\rset^d\eqsp;\eqsp (\widehat \param_{n} -\param)^\top L^\top(L(X^\top X)^{-1}L^\top)^{-1}L(\widehat \param_{n} -\param)\leqslant q\widehat\sigma^2_{n}f_{1-\alpha}^{q,n-d}\right\}
\]
 is a confidence region for $\param_{\star}$ with confidence level $1-\alpha$. The result of Proposition~\ref{prop:Lbeta} may also be used to perform the test 
\[
\mathrm{H}_0:\eqsp L\param_{\star} = \bar\param \quad\mathrm{vs} \quad \mathrm{H}_1:\eqsp L\param_{\star} \neq \bar \param\eqsp,
\]
for a given $\bar \param \in\rset^d$.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%ù%




