\chapter{Technical results}
\minitoc

\section{Refresher}
Let $(\Omega, \calF, \bP)$ be a probability space.
\begin{shaded}
\begin{definition}[conditional expectation]
\label{def:cond:exp}
Let $X$ be a nonnegative random variable and $\mathcal{G}$ be a sub-$\sigma$-field of $\calF$. There exists a nonnegative and $\mathcal{G}$-measurable random variable $Y$ such that, for  all nonnegative and $\mathcal{G}$-measurable random variables $Z$,
\begin{equation}
\label{eq:cond:exp}
\bE[XZ] = \bE[YZ]\eqsp.
\end{equation}
\end{definition}
\end{shaded}
\begin{remark}
If $\bE[X]<\infty$, then $\bE[Y]<\infty$. If $\widetilde Y$ is another nonnegative and $\mathcal{G}$-measurable random variable satisfying \eqref{eq:cond:exp}, then $\widetilde Y = Y$ $\bP$-a.s.
\end{remark}
A random variable $Y$ satisfying the assumptions of Definition~\ref{def:cond:exp} is called (a verion of) the conditional espectation of $X$ given $\mathcal{G}$ and written $\bE[X|\mathcal{G}]$. For all measurable sets $A$, we also write $\bE[\1_A|\mathcal{G}] = \bP(A|\mathcal{G})$. For all random variables $X$ define
$$
\bE[X|\mathcal{G}] = \bE[X_+|\mathcal{G}] - \bE[X_-|\mathcal{G}]\eqsp,
$$
where $X_+ = \mathrm{max}(0,X)$ and $X_- = \mathrm{max}(-X,0)$ are nonnegative random variables.

\begin{shaded}
\begin{proposition}
\label{prop:cond:exp}
Let $X$ be a random variable such that $\bE[X_-|\mathcal{G}]<\infty$ and $\mathcal{G}$ be a sub-$\sigma$-field of $\calF$. Then, $\bP$-a.s., the following properties hold.
\begin{itemize}
\item  If $\mathcal{H}$ is a sub-$\sigma$-field of $\mathcal{G}$, then $\bE[X|\mathcal{G}] = \bE[\bE[X|\mathcal{H}]|\mathcal{G}]$.
\item If $\mathcal{G} = \{\emptyset,\Omega\}$, $\bE[X|\mathcal{G}]=\bE[X]$.
\item If $X$ is independent of $\mathcal{G}$, then $\bE[X|\mathcal{G}]=\bE[X]$.
\item  If $X$ is $\mathcal{G}$-measurable and $\bE[|XY|]<\infty$ and $\bE[|Y|]<\infty$, then $\bE[XY|\mathcal{G}] = X\bE[Y|\mathcal{G}]$.
\end{itemize}
\end{proposition}
\end{shaded}




\section{Probabilistic inequalities}
\begin{shaded}
\begin{theorem}[Hoeffding's inequality]
\label{th:hoeffding}
Let $(X_i)_{1\leqslant i\leqslant n}$ be $n$ independent random variables such that for all $1\leqslant i\leqslant n$, $\bP(a_i\leqslant X_i\leqslant b_i) = 1$ where $a_i, b_i$ are real numbers such that $a_i<b_i$. Then, for all $t>0$,
\[
\bP\left(\left|\sum_{i=1}^n X_i - \sum_{i=1}^n \bE\left[X_i\right]\right|>t\right)\leqslant 2\mathrm{exp}\left(\frac{-2t^2}{\sum_{i=1}^n(b_i-a_i)^2}\right)\eqsp.
\]
\end{theorem}
\end{shaded}
\begin{proof}
Without loss of generality, assume that $\PE[X_i]=0$ for all $1\leqslant i\leqslant n$. It is enough to prove that, for all $t>0$,
\begin{equation}
\label{eq:hoef}
\bP\lr{\sum_{i=1}^n X_i>t}\leqslant \mathrm{exp}\left(\frac{-2t^2}{\sum_{i=1}^n(b_i-a_i)^2}\right)\eqsp.
\end{equation}
Equation \eqref{eq:hoef} implies Hoeffding's inequality by noting that $\bP\lr{|\sum_{i=1}^n X_i|>t}\leq \bP\lr{\sum_{i=1}^n X_i>t}+\bP\lr{-\sum_{i=1}^n X_i>t}$ and by applying \eqref{eq:hoef} to $(X_i)_{1\leqslant i\leqslant n}$ and $(-X_i)_{1\leqslant i\leqslant n}$.
 Write, for any $s,t>0$,
\begin{align*}
\PP\lr{\sum_{i=1}^n X_i>t}=\PP\lr{\rme^{s \sum_{i=1}^n X_i}>\rme^{st}} < \rme^{-st}\PE\lrb{\rme^{s \sum_{i=1}^n X_i}}=\rme^{-st}\prod_{i=1}^{n}\PE\lrb{\rme^{s X_i}}
\end{align*}
To bound the right hand side of this inequality, set, for all $1\leqslant i\leqslant n$, $\phi_i: s\mapsto \log\lr{\PE\lrb{\rme^{s X_i}}}$. Since $X_i$ is almost surely bounded,  $\phi_i$ is differentiable  and for all $s>0$, $\phi_i'(s)=\PE\lrb{X_i \rme^{s X_i}}/\PE\lrb{\rme^{s X_i}}$. Then, differentiating again,
\begin{align*}
\phi_i''(s)=\log''\lr{\PE\lrb{\rme^{s X_i}}}=\frac{\PE\lrb{X_i^2 \rme^{s X_i}}}{\PE\lrb{\rme^{s X_i}}} - \lr{\frac{\PE\lrb{X_i \rme^{s X_i}}}{\PE\lrb{\rme^{s X_i}}}}^2=\widetilde \PE_i[X^2]-(\widetilde \PE_i[X])^2 =\widetilde \PE_i[(X-\widetilde \PE_i[X])^2]\eqsp,
\end{align*}
where
\[
\widetilde \PE_i[Z]=\frac{\PE\lrb{Z \rme^{s X_i}}}{\PE\lrb{\rme^{s X_i}}}\eqsp.
\]
Then,
\[
\phi_i''(s) = \inf_{x \in [a_i,b_i]}\widetilde \PE_i[(X-x)^2] \leqslant \widetilde \PE_i\lrb{\lr{X-\frac{a_i+b_i}{2}}^2} \leqslant \lr{\frac{b_i-a_i}{2}}^2\eqsp.
\]
Finally, using Taylor's expansion,
\begin{equation}
\label{eq:hoeffding:loglaplace}
\phi_i(s)\leq \phi_i(0)+\phi_i'(0)+\frac{s^2}{2} \sup_{\alpha \in [0,1]} \phi_i''(\alpha s) \leq \frac{s^2(b_i-a_i)^2}{8}\eqsp.
\end{equation}
This implies
$$
\PP\lr{\sum_{i=1}^n X_i>t} \leqslant \rme^{-st} \rme^{s^2 \sum_{i=1}^n  \frac{(b_i-a_i)^2}{8}} \eqsp.
$$
Choosing $s=4t/(\sum_{i=1}^n (b_i-a_i)^2)$ minimizes the right hand side and yields \eqref{eq:hoef}.


\end{proof}

\begin{shaded}
\begin{lemma}
\label{lem:bernoulli:bound}
Let $X$ be a Bernoulli random variable. Then, for all $t>0$,
\[
\Psi(t) = \bE\left[\rme^{t\left(X-\bE[X]\right)}\right] \leqslant \rme^{t^2/8}\eqsp.
\]
\end{lemma}
\end{shaded}

\begin{proof}
Let $p\in(0,1)$ be such that $p = \bP(X=1)$ (cases $p=0$ and $p=1$ are straightforward). For all $t>0$,
\[
\varphi(t) = \log \Psi(t) = \log\left(1-p+p\rme^t\right) - pt\eqsp.
\]
The proof then follows from proof of the Hoeffding inequality, i.e. \eqref{eq:hoeffding:loglaplace} with $b_i =1-p$ and $a_i = -p$.
%The function $\Psi$ is twice continuously differentiable and such that for all $t>0$,
%\[
%\varphi'(t) = \frac{p\rme^{t}}{1-p+p\rme^t} - p\quad\mathrm{and}\quad |\varphi''(t)| =\frac{p\rme^{t}(1-p+p\rme^t) - (p \rme^t)^2}{(1-p+p\rme^t)^2} = \frac{p\rme^{t}(1-p)}{(1-p+p\rme^t)^2} \leqslant \frac{1}{4} \eqsp.
%\]
%Then, by a second order Taylor expansion, for all $t>0$,
%\[
%\varphi(t) \leqslant \varphi(0) + t\varphi'(0) + \frac{t^2}{8}\leqslant \frac{t^2}{8}\eqsp,
%\]
%which concludes the proof.
\end{proof}


%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{Matrix calculus}
\begin{shaded}
\begin{lemma}
Let $X$ be a random vector in $\mathbb{R}^d$ with covariance matrix $\Sigma$ and $A$ be a real matrix in $\mathbb{R}^{m\times d}$. Then, $\bV[AX] = A\Sigma A^\top$.
\end{lemma}
\end{shaded}
\begin{proof}
By definition, using the linearity of the expectation,
$$
\bV[AX] = \bE\left[(AX - \bE[AX])(AX - \bE[AX])^\top\right] = \bE\left[A(X - \bE[X])(X - \bE[X])^\top A^\top\right] = A\bE\left[(X - \bE[X])(X - \bE[X])^\top \right]A^\top = A\Sigma A^\top\eqsp.
$$
\end{proof}

\begin{shaded}
\begin{lemma}
\label{ref:expectation:quadratic:trasform}
Let $X$ be a random vector in $\rset^d$ with mean $\mu\in\rset^d$ and covariance matrix $\Sigma\in\rset^{d\times d}$ and $A$ a symmetric matrix in $\rset^{d\times d}$. Then,
$$
\bE[X^\top A X] = \mu^\top A \mu + \mathrm{Trace}(A\Sigma)\eqsp.
$$
\end{lemma}
\end{shaded}
\begin{proof}
As $X^\top A X$ is a real number, $\bE[X^\top A X] = \bE[\mathrm{Trace}(X^\top A X)] =  \bE[\mathrm{Trace}(AXX^\top )]$. By linearity, $\bE[X^\top A X] = \mathrm{Trace}(A\bE[XX^\top])$ which yields,
$$
\bE[X^\top A X] =   \mathrm{Trace}(A\{\bV[X] + \bE[X]\bE[X]^\top\}) =  \mu^\top A \mu + \mathrm{Trace}(A\Sigma)\eqsp.
$$
\end{proof}

\begin{lemma}
\label{lem:im:transpose}
Let $A$ be a $n\times d$ matrix with real entries. Then, $\mathrm{range}(A)=\mathrm{range}(A A^\top)$.
\end{lemma}
\begin{proof}
First note that for all $x\in\rset^n$, $AA^\top x=0$ implies $\langle A^\top x; A^\top x\rangle=0$ so that $A^\top x=0$. The converse is obvious. Therefore, $\mathrm{Ker}(A A^\top)= \mathrm{Ker}(A^\top)$. Using that for any matrix $B$, $\mathrm{Ker}(B^\top)=(\mathrm{range}(B))^\perp$, yields $\mathrm{range}(A A^\top)^\perp=\mathrm{range}(A)^\perp$, which concludes the proof.
\end{proof}

\begin{lemma}
\label{lem:ortho:proj}
Let $\{U_k\}_{1\leqslant k \leqslant r}$ be a family of $r$ orthonormal vectors of $\rset^d$.
Then, $\sum_{k=1}^{r} U_k U_k^\top$ is the matrix of the orthogonal projection onto 
$$
\mathbf{H}=\left\{\sum_{k=1}^r \alpha_k U_k\,; \ \alpha_1,\ldots,\alpha_r \in \rset\right\}\eqsp.
$$ 
\end{lemma}
\begin{remark}
If $A$ is a $n\times d$ matrix with real entries such that each column of $A$ is in $\mathrm{H}$, then,
$$
\left(\sum_{k=1}^r U_k U_k^\top\right) A = A\eqsp.
$$
\end{remark}
\begin{proof}
For all $X\in\rset^d$, let $\pi_\mathbf{H}(X)$ be the orthogonal projection of $X$ onto $\mathbf{H}$. Since $\{U_k\}_{1\leqslant k \leqslant r}$  is an orthonormal basis of $\mathbf{H}$,
$$
\pi_\mathbf{H}(X)=\sum_{k=1}^{r} \langle X; U_k\rangle U_k=\left(\sum_{k=1}^{r} U_k U_k^\top\right)X\eqsp.
$$
This implies in particular that for each $X \in \mathbf{H}$, $X=\left(\sum_{k=1}^{r} U_k U_k^\top\right)X$. %Since all the column vectors of $A$ are in $\mathbf{H}$, this yields $\left(\sum_{k=1}^r U_k U_k^T\right) A=A$.
\end{proof}

\begin{shaded}
\begin{proposition}[Singular value decomposition] 
\label{prop:acp:svd}
For all $\rset^{n \times d}$ matrix $A$ with rank $r$, there exist $\sigma_1\geqslant \ldots \geqslant \sigma_r>0$ such that
\[
A = \sum_{k=1}^r \sigma_k u_k v^\top_k\eqsp,
\]
where $\{u_1,\ldots,u_r\}\in (\rset^n)^r$ and $\{v_1,\ldots,v_r\}\in (\rset^d)^r$ are two orthonormal families. The vectors $\{\sigma_1,\ldots,\sigma_r\}$ are called singular values of $A$ and $\{u_1,\ldots,u_r\}$ (resp. $\{v_1,\ldots,v_r\}$) are the left-singular (resp. right-singular) vectors of $A$.
\end{proposition}
\end{shaded}
\begin{remark}
If $U$ denotes the $\rset^{n\times r}$ matrix with columns given by $\{u_1,\ldots,u_r\}$ and $V$ denotes the $\rset^{p \times r}$ matrix with columns given by $\{v_1,\ldots,v_r\}$, then the singular value decomposition of $A$ may also be written as
\[
A = UD_rV^\top\eqsp,
\]
where $D_r = \mathrm{diag}(\sigma_1,\ldots,\sigma_r)$.
\end{remark}
\begin{remark}
The singular value decomposition is closely related to the spectral theorem for symmetric semipositive definite matrices. In the framework of Proposition~\ref{prop:acp:svd}, $A^\top A$ and $AA^\top$ are positive semidefinite such that
\[
A^\top A = VD_r^2V^\top\quad\mathrm{and}\quad AA^\top = UD_r^2U^\top\eqsp.
\]
\end{remark}
\begin{proof}
Since the matrix $AA^\top$ is positive semidefinite, its spectral decomposition is given by
\[
AA^\top = \sum_{k=1}^r \lambda_k u_k u^\top_k\eqsp,
\]
where $\lambda_1\geqslant \ldots\geqslant \lambda_r>0$ are the nonzero eigenvalues of $AA^\top$ and $\{u_1,\ldots,u_r\}$ is an orthonormal family of $\rset^n$. For all $1\leqslant k\leqslant r$, define $v_k = \lambda_k^{-1/2}A^\top u_k$ so that
\begin{align*}
\|v_k\|^2&=\lambda_k^{-1}\langle A^\top u_k;A^\top u_k\rangle = \lambda_k^{-1} u^\top_kAA^\top u_k = 1\eqsp, \\
A^\top Av_k & = \lambda_k^{-1/2}A^\top A A^\top u_k  = \lambda_k v_k\eqsp.
\end{align*}
On the other hand, for all $1\leqslant k\neq j\leqslant r$, $\langle v_k;v_j\rangle = \lambda_k^{-1/2}\lambda_j^{-1/2}u^\top_kA A^\top u_j =\lambda_k^{-1/2}\lambda_j^{1/2}u^\top_ku_j = 0$. Therefore, $\{v_1,\ldots,v_r\}$ is an orthonormal family of eigenvectors of $A^\top A$ associated with the eigenvalues $\lambda_1\geqslant \ldots\geqslant \lambda_r>0$. 
Define, for all $1\leqslant k\leqslant r$, $\sigma_k = \lambda_k^{1/2}$ which yields
\[
\sum_{k=1}^r \sigma_k u_k v^\top_k = \sum_{k=1}^r  u_k u^\top_kA = \left(\sum_{k=1}^r  u_k u^\top_k\right)A\eqsp.
\]
As $\{u_1,\ldots,u_r\}$ is an orthonormal family, by Lemma~\ref{lem:ortho:proj}$UU^\top = \sum_{k=1}^r u_ku^\top_k$ is the orthogonal projection onto the $\mathrm{range}(AA^\top)$. And, by Lemma~\ref{lem:im:transpose},  $\mathrm{range}(AA^\top)= \mathrm{range}(A)$, which implies
\[
\sum_{k=1}^r \sigma_k u_k v^\top_k = \left(\sum_{k=1}^r  u_k u^\top_k\right)A = A\eqsp.
\]
\end{proof}

\noindent Let $\Mplus$ the space of real-valued $d \times d$ symmetric positive matrices.

\begin{lemma}
The function $\Sigma \mapsto \log \det \Sigma$ is concave on $\Mplus$.
\end{lemma}
\begin{proof}
Let $\Sigma,\Gamma \in \Mplus$ and $\lambda \in [0,1]$.  Since $\Sigma^{-1/2}\Gamma\Sigma^{-1/2} \in \Mplus$, it is diagonalisable in some orthonormal basis and write $\mu_1,\ldots, \mu_d$ the (possibly repeated) entries of the diagonal. Note in particular that $\det \lr{\Sigma^{-1/2}\Gamma\Sigma^{-1/2}}=\prod_{i=1}^d \mu_i$. Then,
\begin{align*}
\log \det \lr{(1-\lambda)\Sigma+\lambda \Gamma}&=\log \det \lrb{\Sigma^{1/2} \lr{(1-\lambda)I+\lambda \Sigma^{-1/2}\Gamma\Sigma^{-1/2}} \Sigma^{1/2}}\\
&=\log \det \Sigma + \log \det \lr{(1-\lambda)I+\lambda \Sigma^{-1/2}\Gamma\Sigma^{-1/2}} \nonumber \\
&=\log \det \Sigma + \sum_{i=1}^d \log(1-\lambda+\lambda \mu_i)\nonumber \\
& \geq \log \det \Sigma + \sum_{i=1}^d (1-\lambda) \log(1)+\lambda \log( \mu_i) \label{eq:diag}:= D
\end{align*}
where the last inequality follows from the concavity of the $\log$. Now, rewrite the rhs $D$ as:
\begin{align*}
D&=(1-\lambda) \log \det \Sigma + \lambda \lr{\log \det \Sigma^{1/2}+ \log \det \Sigma^{-1/2}\Gamma\Sigma^{-1/2} + \log \det \Sigma^{1/2}} \\
&=(1-\lambda) \log \det \Sigma + \lambda \log \det \Gamma
\end{align*}
This finishes the proof.
\end{proof}






\begin{lemma}
\label{lem:matrix:calculus}
Let $\Sigma$ be a symmetric and invertible  matrix in $\rset^{d\times d}$.
\begin{enumerate}[(i)]
\item The derivative of the real valued function $\Sigma \mapsto \log\mathrm{det}(\Sigma)$ defined on $\rset^{d\times d}$ is given by:
\[
\partial_{\Sigma}\{\log\mathrm{det}(\Sigma)\}= \Sigma^{-1}\eqsp,
\]
where, for all real valued function $f$ defined on $\rset^{d\times d}$, $\partial_{\Sigma}f(\Sigma)$ denotes the $\rset^{d\times d}$ matrix such that for all $1\leqslant i,j\leqslant d$, $\{\partial_{\Sigma}f(\Sigma)\}_{i,j}$ is the partial derivative of $f$ with respect to $\Sigma_{i,j}$.
\item The derivative of the real valued  function  $x\mapsto x^\top\Sigma x$ defined on $\rset^d$ is given by:
\[
\partial_{x}\{x^\top\Sigma x\} = 2 \Sigma x\eqsp.
\]
\end{enumerate}
\end{lemma}
\begin{proof}
  \begin{enumerateList}
    \item Recall that for all $i \in \{1,\ldots,d\}$ we have
      $\det(\Sigma)=\sum_{k=1}^d \Sigma_{i,k} \Delta_{i,k}$ where
      $\Delta_{i,j}$ is the $(i,j)$-cofactor associated to
      $\Sigma$. For any fixed $i,j$, the component $\Sigma_{i,j}$ does not appear in anywhere in
      the decomposition $\sum_{k=1}^d \Sigma_{i,k} \Delta_{i,k}$,
      except for the term $k=j$. This implies
      $$
      \frac{\partial \log \det(\Sigma)}{\partial \Sigma_{i,j}}=
      \frac{1}{\det \Sigma}\frac{\partial  \det(\Sigma)}{\partial
        \Sigma_{i,j}}=\frac{\Delta_{i,j}}{\det  \Sigma}
      $$
      Recalling the identity $\Sigma\; [\Delta_{j,i}]_{1\leq i,j \leq d}=(\det
      \Sigma)\; I_d$ so that $\Sigma^{-1}=\frac{[\Delta_{j,i}]_{1\leq i,j \leq d}^\top}{\det
      \Sigma}$, we finally get
      $$
\lrb{\frac{\partial \log \det(\Sigma)}{\partial \Sigma_{i,j}}}_{1\leq
  i,j  \leq d}=(\Sigma^{-1})^\top=\Sigma^{-1}
$$
where the last equality follows from the fact that $\Sigma$ is
symmetric.
\item Define $\varphi(x)=x^\top\Sigma x$. Then, by straightforward
  algebra, $\varphi(x+h)=\varphi(x)+2h^\top\Sigma
  x+\varphi(h)=\varphi(x)+2h^\top\Sigma x+o(\|h\|)$, which concludes the
  proof.
  \end{enumerateList}
\end{proof}
