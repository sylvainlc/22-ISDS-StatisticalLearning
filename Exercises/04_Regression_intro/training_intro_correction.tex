\documentclass[a4paper,10pt,fleqn]{article}

\usepackage{a4wide,amsmath,amsthm,amssymb,bbm,fancyhdr}
\usepackage{ifthen,color,enumerate,comment,dsfont,pdfsync,framed,todonotes,enumitem}

\newcommand{\titre}[1]{\textbf{\textsc{#1}}}

\RequirePackage[T1]{fontenc}

\usepackage[latin1]{inputenc}
\usepackage{graphicx}
\usepackage{dsfont}
\usepackage{enumitem}
\newcommand{\eqsp}{\,}
\newcommand{\R}{\ensuremath{\mathbb{R}}}
\newcommand{\calF}{\mathcal{F}}
\newcommand{\rmd}{\mathrm{d}}
\newcommand{\N}{\mathbb{N}}
\newcommand{\rset}{\ensuremath{\mathbb{R}}}
\renewcommand{\P}{\ensuremath{\operatorname{P}}}
\newcommand{\bP}{\mathbb{P}}
\newcommand{\E}{\ensuremath{\mathbb{E}}}
\newcommand{\rme}{\ensuremath{\mathrm{e}}}
\newcommand{\calH}{\ensuremath{\mathcal{H}}}
\newcommand{\xset}{\ensuremath{\mathsf{X}}}
\newcommand{\V}{\ensuremath{\mathbb{V}}}
\newcommand{\Sb}{\ensuremath{\mathbb{S}}}
\newcommand{\gaus}{\ensuremath{\mathcal{N}}}
\newcommand{\HH}{\ensuremath{\mathcal{H}}}
\newcommand{\F}{\ensuremath{\mathcal{F}}}
\newcommand{\W}{\ensuremath{\mathcal{W}}}
\newcommand{\X}{\ensuremath{\mathcal{X}}}
\newcommand{\1}{\ensuremath{\mathbbm{1}}}
\newcommand{\dlim}{\ensuremath{\stackrel{\mathcal{L}}{\longrightarrow}}}
\newcommand{\plim}{\ensuremath{\stackrel{\mathrm{P}}{\longrightarrow}}}
\newcommand{\PP}{\ensuremath{\mathbb{P}}}
\newcommand{\p}{\ensuremath{\mathbb{P}}}
\newcommand{\eps}{\varepsilon}
\newcommand{\bE}{\mathbb{E}}

\newcommand{\pa}[1]{\left(#1\right)}
\newcommand{\hatk}{\widehat K}
\newcommand{\f}{\varphi}
\newcommand{\Id}{\textsf{Id}}
\newcommand{\bfU}{\mathbf{U}}
\newcommand{\bfX}{\mathbf{X}}
\newcommand{\bfs}{\mathbf{\Sigma}}
\newcommand{\bfA}{\mathbf{A}}
\newcommand{\bfV}{\mathbf{V}}
\newcommand{\bfB}{\mathbf{B}}
\newcommand{\bfI}{\mathbf{I}}
\newcommand{\bfD}{\mathbf{D}}
\newcommand{\bfK}{\mathbf{K}}
\newcommand{\argmin}{\mathop{\textrm{argmin}}}
\newcommand{\argmax}{\mathop{\textrm{argmax}}}
\newcommand{\crit}{\mathop{\textrm{crit}}}
\newcommand{\C}{\mathcal{C}}
\newcommand{\pc}{\pi_{\mathcal{C}}}


% Style


\begin{document}

%\noindent EMINES \hfill \\%ISUP - Sorbonne Universit\'e \\
% 2023-2023

\noindent\hrulefill

\begin{center}
\textsc{Regression: introduction}
\end{center}
\hrulefill

\medskip


\section*{Gaussian vectors}
\begin{enumerate}
\item Let $\Sigma$ be a symmetric positive definite matrix of $\rset^{n\times n}$. Provide a solution to sample a Gaussian vector with covariance matrix $\Sigma$ based on i.i.d. standard Gaussian variables.

\vspace{.2cm}

{\em
It is enough to remark that $X = \mu +\Sigma^{1/2}\varepsilon \sim\mathcal{N}(\mu,\Sigma)$ where $\mu\in\rset^d$ and $\varepsilon \sim \mathcal{N}(0,I_d)$.
}

\item Let $\varepsilon$ be a random variable in $\{-1,1\}$ such that $\bP(\varepsilon = 1) = 1/2$. If $(X,Y)^\top\sim \mathcal{N}(0,I_2)$ explain why the following vectors are or are not Gaussian vectors.
\begin{enumerate}
\item $(X,\varepsilon X)$\eqsp.

{\em
Not Gaussian since the probability that  $X +\varepsilon X = 0$ is $1/2$.
}
\item $(X,\varepsilon Y)$\eqsp.

{\em
Gaussian since coordinates are independent Gaussian random variables.
}
\item $(X,\varepsilon X + Y)$\eqsp.

{\em
Not Gaussian since the characteristic function of $(1+\varepsilon) X + Y$ is not the Gaussian characteristic function.
}
\item $(X,X + \varepsilon Y)$\eqsp.

{\em
Gaussian as a linear transform of $(b)$. Indeed,
$$
\begin{pmatrix} X \\X + \varepsilon Y\end{pmatrix} = \begin{pmatrix} 1 && 0 \\1 && 1\end{pmatrix}\begin{pmatrix} X \\ \varepsilon Y\end{pmatrix}\eqsp.
$$
}
\end{enumerate}


\item Let $X$ be a Gaussian vector in $\rset^n$ with mean $\mu\in\rset^n$ and covariance matrix $\sigma^2 I_n$. Prove that the random variables $\bar X_n$ and $\widehat \sigma^2_n$ defined as
$$
\bar X_n = \frac{1}{n}\sum_{i=1}^n X_i\quad \mathrm{and} \quad \widehat \sigma^2_n = \frac{1}{n-1}\sum_{i=1}^n (X_i - \bar X_n)^2
$$
are independent.

\vspace{.2cm}

{\em
Let $\1_n$ the vector of $\rset^n$ with all entries equal to 1. Then, $\bar X_n = n^{-1}\1_n^\top X$ and $(n-1) \sigma^2_n = \|X - \bar X_n\1_n\|_2^2 = \|X - n^{-1}\1_n\1_n^\top X\|_2^2 = \|(I_n -  (n^{-1/2}\1_n)(n^{-1/2}\1_n)^\top) X\|_2^2$. Note that $(n^{-1/2}\1_n)(n^{-1/2}\1_n)^\top$ is the orthogonal projection onto $\mathrm{span}(\1_n)$ and $I_n -  (n^{-1/2}\1_n)(n^{-1/2}\1_n)^\top$ onto its orthogonal. The proof is completed by using Cochran's theorem.
}
\end{enumerate}


\section*{Regression: prediction of a new observation}
Consider the regression model given by
$$
Y = X\beta_{\star}+ \xi\eqsp,
$$
where $X\in\rset^{n\times d}$ the $(\xi_{i})_{1\leqslant i \leqslant n}$ are i.i.d. centered Gaussian random variables with variance $\sigma_{\star}^2$. Assume that $X^\top X$ has full rank and that $\beta_\star$ and $\sigma_{\star}^2$ are estimated by 
$$
\widehat \beta_n = (X^\top X)^{-1}X^\top Y\quad\mathrm{and}\quad \widehat \sigma^2_n =\frac{\|Y - X\widehat \beta_n \|^2}{n-d}\eqsp.
$$
Let $x_\star \in\rset^d$ and assume that its associated observation $Y_\star = x_\star^\top\beta_\star + \varepsilon_\star$ is predicted by $\widehat Y_\star = x_\star^\top\widehat \beta_n$.
\begin{enumerate}
\item  Provide the expression of $\bE[(\widehat Y_\star - x_\star^\top\beta_\star)^2]$.

\vspace{.2cm}

{\em
By definition of $\widehat \beta_n $,
$$
\widehat Y_\star - x_\star^\top\beta_\star = x_\star^T (\widehat\beta_n - \beta_\star)\,,
$$
so that $\bE[\widehat Y_\star] =  x_\star^\top\beta_\star$ and
$$
\bE[(\widehat Y_\star - x_\star^T\beta_\star)^2] = \mathbb{V}[\widehat Y_\star] = x_\star^\top \mathbb{V}[\widehat\beta_n]x_\star\,.
$$
On the other hand,
$$
\mathbb{V}[\widehat\beta_n] = (X^\top X)^{-1}X^\top \mathbb{V}[Y] X(X^TX)^{-1} = \sigma^2(X^\top X)^{-1}\,.
$$
Therefore,
$$
\bE[(\widehat Y_\star - x_\star^\top\beta_\star)^2] = \sigma^2x_\star^\top (X^\top X)^{-1}x_\star\,.
$$
}

\item  Provide a confidence interval for $x_\star^\top\beta_\star$ with statistical significiance $1-\alpha$ for $\alpha\in(0,1)$.

\vspace{.2cm}

{\em
By the first question, $\widehat Y_\star$ is a Gaussian random variable with mean $x_\star^\top\beta_\star$ and variance $\sigma_\star^2x_\star^\top(X^\top X)^{-1}x_\star$. If $z_{1-\alpha/2}$ is the quantile of order $1-\alpha/2$ of the standard Gaussian variable,
$$
\mathbb{P}\left(\frac{\left|\widehat Y_\star-x_\star^\top\beta_\star\right|}{\sigma_\star(x_\star^\top(X^\top X)^{-1}x_\star)^{1/2}}\leqslant z_{1-\alpha/2}\right)\geqslant 1-\alpha\,.
$$
Therefore, with probability larger than $1-\alpha$,
$$
x_\star^\top\beta_\star \in \left(\widehat Y_\star - \sigma_\star(x_\star^\top (X^\top X)^{-1}x_\star)^{1/2}z_{1-\alpha/2}\,;\, \widehat Y_\star + \sigma_\star(x_\star^\top(X^\top X)^{-1}x_\star)^{1/2}z_{1-\alpha/2}\right)\,.
$$
}
\end{enumerate}


\section*{Regression: linear estimators}
Consider the regression model given, for all $1\leqslant i\leqslant n$, by
$$
Y_{i}=f^*(X_{i})+\xi_{i}\eqsp,
$$
where for all $1\leqslant i\leqslant n$, $X_i\in\xset$, and the $(\xi_{i})_{1\leqslant i \leqslant n}$ are i.i.d. centered Gaussian random variables with variance $\sigma^2$. In this exercise, $f^*$ is estimated by a linear estimator of the form
$$
\widehat f_n: x \mapsto \sum_{i = 1}^n w_i(x)Y_i\eqsp.
$$
Prove that
$$
\frac{1}{n}\bE\left[\sum_{i=1}^n(\widehat f_n(X_i) - f^*(X_i))^2\right] = \frac{1}{n}\|Wf^*(X) - f^*(X)\|_2^2 + \frac{\sigma^2}{n}\mathrm{Trace}(W^\top W)\eqsp, 
$$
where $W = (w_i(X_j))_{1\leqslant i,j \leqslant n}$ and $f^*(X) = (f^*(X_1),\ldots,f^*(X_n))^\top$.

\vspace{.2cm}

{\em
Note that
$$
\frac{1}{n}\bE\left[\sum_{i=1}^n(\widehat f_n(X_i) - f^*(X_i))^2\right] = \frac{1}{n}\bE\left[\|WY - f^*(X)\|_2^2\right]\eqsp,
$$
where $Y = (Y_1,\ldots,Y_n)^\top$. then, write
\begin{multline*}
\bE\left[\|WY - f^*(X)\|_2^2\right] = \bE\left[\|WY - Wf^*(X)\|_2^2\right] + \bE\left[\|Wf^*(X) - f^*(X)\|_2^2\right]\\ + 2\bE\left[\langle WY - Wf^*(X); Wf^*(X) - f^*(X)\rangle\right]\eqsp.
\end{multline*}
As $\bE[Y] = f^*(X)$, this yields
$$
\bE\left[\|WY - f^*(X)\|_2^2\right] = \bE\left[\|WY - Wf^*(X)\|_2^2\right] + \|Wf^*(X) - f^*(X)\|_2^2\eqsp.
$$
The proof is completed by noting that
$$
 \bE\left[\|WY - Wf^*(X)\|_2^2\right] =  \bE\left[(Y - f^*(X))^\top W^\top W(Y - f^*(X))\right] = \mathrm{Trace}\left(  W^\top W \mathbb{V}[Y - f^*(X)]\right)
$$
and $\mathbb{V}[Y - f^*(X)] = \sigma^2 I_n$.
}
\end{document}





	
