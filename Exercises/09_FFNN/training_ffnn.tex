\documentclass[a4paper,10pt,fleqn]{article}

\usepackage{a4wide,amsmath,amsthm,amssymb,bbm,fancyhdr}
\usepackage{ifthen,color,enumerate,comment,dsfont,pdfsync,framed,todonotes,enumitem}
\newboolean{corrige}
\setboolean{corrige}{true}

\newcommand{\titre}[1]{\textbf{\textsc{#1}}}

\RequirePackage[T1]{fontenc}

\usepackage[latin1]{inputenc}
\usepackage{graphicx}
\usepackage{dsfont}
\newcommand{\thisyear}{}
\usepackage{enumitem}
\newcommand{\eqsp}{\,}
\newcommand{\R}{\ensuremath{\mathbb{R}}}
\newcommand{\calF}{\mathcal{F}}
\newcommand{\N}{\mathbb{N}}
\newcommand{\rset}{\ensuremath{\mathbb{R}}}
\renewcommand{\P}{\ensuremath{\operatorname{P}}}
\newcommand{\bP}{\mathbb{P}}
\newcommand{\E}{\ensuremath{\mathbb{E}}}
\newcommand{\rme}{\ensuremath{\mathrm{e}}}
\newcommand{\calH}{\ensuremath{\mathcal{H}}}
\newcommand{\xset}{\ensuremath{\mathsf{X}}}
\newcommand{\V}{\ensuremath{\mathbb{V}}}
\newcommand{\Sb}{\ensuremath{\mathbb{S}}}
\newcommand{\gaus}{\ensuremath{\mathcal{N}}}
\newcommand{\HH}{\ensuremath{\mathcal{H}}}
\newcommand{\F}{\ensuremath{\mathcal{F}}}
\newcommand{\W}{\ensuremath{\mathcal{W}}}
\newcommand{\X}{\ensuremath{\mathcal{X}}}
\newcommand{\1}{\ensuremath{\mathbbm{1}}}
\newcommand{\dlim}{\ensuremath{\stackrel{\mathcal{L}}{\longrightarrow}}}
\newcommand{\plim}{\ensuremath{\stackrel{\mathrm{P}}{\longrightarrow}}}
\newcommand{\PP}{\ensuremath{\mathbb{P}}}
\newcommand{\p}{\ensuremath{\mathbb{P}}}
\newcommand{\eps}{\varepsilon}
\newcommand{\bE}{\mathbb{E}}
\newcommand{\pa}[1]{\left(#1\right)}
\newcommand{\hatk}{\widehat K}
\newcommand{\f}{\varphi}
\newcommand{\Id}{\textsf{Id}}
\newcommand{\bfU}{\mathbf{U}}
\newcommand{\bfX}{\mathbf{X}}
\newcommand{\bfs}{\mathbf{\Sigma}}
\newcommand{\bfA}{\mathbf{A}}
\newcommand{\bfV}{\mathbf{V}}
\newcommand{\bfB}{\mathbf{B}}
\newcommand{\bfI}{\mathbf{I}}
\newcommand{\bfD}{\mathbf{D}}
\newcommand{\bfK}{\mathbf{K}}
\newcommand{\argmin}{\mathop{\textrm{argmin}}}
\newcommand{\argmax}{\mathop{\textrm{argmax}}}
\newcommand{\crit}{\mathop{\textrm{crit}}}
\newcommand{\C}{\mathcal{C}}
\newcommand{\pc}{\pi_{\mathcal{C}}}


% Style
%\pagestyle{fancyplain}
\renewcommand{\sectionmark}[1]{\markright{#1}}
\renewcommand{\subsectionmark}[1]{}
%\lhead[\fancyplain{}{\thepage}]{\fancyplain{}{\footnotesize {\sf
%MAT4506 Introduction to Machine Learning  %/ \rightmark
%}}}
%\rhead[\fancyplain{}{\footnotesize {\sf MAT4506 Introduction to machine learning, \thisyear %/ \rightmark
%}}]{\fancyplain{}{\thepage}}


\newtheorem{theorem}{Theorem}

%% Titre
%\title{{\bf Machine learning}}
%\author{{\em Logistic regression}}
%\date{}


\begin{document}

%\noindent Machine learning \hfill ISUP - Sorbonne Universit\'e \\
% 2022-2023

\noindent\hrulefill

\begin{center}
\textsc{Feed Forward Neural Networks}
\end{center}
\hrulefill

\medskip



\section*{Warm-up}
Assume that the observation $Y$ takes values in $\{1,\ldots,M\}$ and that $X\in\mathbb{R}^d$. The negative loglikelihood to be minimized to estimate the parameters of the model is given by:
$$
\theta \mapsto \ell^{\mathrm{multi}}_n(\theta)= -\frac{1}{n} \sum_{i=1}^n\sum_{k=1}^{M} \1_{Y_i=k}\log \mathbb{P}_{\theta}(Y_i = k | X_i)\,,
$$
where $\{(X_i,Y_i)\}_{1\leqslant i\leqslant n}$ are i.i.d. observations with the same law as $(X,Y)$. 

\begin{enumerate}
\item Explain the construction of $\mathbb{P}_{\theta}(Y_i = k | X_i)$, $1\leqslant i\leqslant n$ for the following model. A feed forward neural network with a first hidden layer with dimension $d_1$ and activation function $\varphi_1$,  a second hidden layer with dimension $d_2$ and activation function $\varphi_2$, and an output layer of dimension $M$ and activation function given by the softmax function.
\item What is the unknown parameter $\theta$ of the previous model ? Explain how to estimate $\theta$ with  a stochastic gradient descent.
\item What is the complexity of an iteration of the previous algorithm ?
\end{enumerate}



\section*{Backpropagation}
Let $x\in\rset^d$ be the input of a MLP with L layers and define all layers as follows.
\begin{align*}
h_{\theta}^0(x) &= x\eqsp,\\
z_{\theta}^k(x)  &= b^k + W^kh_{\theta}^{k-1}(x)\quad \mathrm{for\;all\;} 1\leqslant k\leqslant L\eqsp,\\
h_{\theta}^k(x)  &= \varphi_k(z_{\theta}^{k}(x))\quad \mathrm{for\;all\;}1\leqslant k\leqslant L\eqsp,
\end{align*}
where $b^1\in\rset^{d_1}$, $W^1\in\rset^{d_1\times d}$ and for all $2\leqslant k\leqslant L$, $b^k\in\rset^{d_k}$, $W^k\in\rset^{d_k\times d_{k-1}}$. For all $1\leqslant k\leqslant L$, $\varphi_k: \rset^{d_k} \to \rset^{d_k}$ is a nonlinear activation function. Let $\theta = \{b^1,W^1,\ldots,b^L,W^L\}$ be the unknown parameters of the MLP and 
$$
f_{\theta}(x) = h_{\theta}^L(x)
$$
 be the output layer of the MLP. As there is no modelling assumptions anymore, virtually any activation functions $\varphi^m$, $1\leqslant m\leqslant L-1$ may be used. In this section, it is assumed that these intermediate activation functions apply elementwise and, with a minor abuse of notations, we write for all $1\leqslant m\leqslant L-1$ and all $z\in\rset^{d_m}$,
$$
\varphi_m(z) = (\varphi_m(z_1),\ldots, \varphi_m(z_{d_m}))\eqsp,
$$
with $\varphi_m: \rset\to \rset$ the seleced scalar activation function. 

In a classification setting ,the output $h_{\theta}^L(x)$ is the estimate of the probability that the class is  $k$ for all $1\leqslant k\leqslant M$, given the input $x$. The common choice in this case is the softmax function: for all $1\leqslant i\leqslant M$
$$
\varphi_L(z)_i = \mathrm{softmax}(z)_i = \frac{\rme^{z_i}}{\sum_{j=1}^M\rme^{z_j}}\eqsp.
$$
In this case $d_L = M$ and each component $k$ of $h_{\theta}^L(x)$ contains $\mathbb{P}(Y=k | X)$.

\begin{enumerate}
\item Prove that  for all $1\leqslant i,j\leqslant M$,
$$
\partial_{z_i}(\varphi_L(z))_j =  \left\{
    \begin{array}{ll}
        \mathrm{softmax}(z)_i (1-\mathrm{softmax}(z)_i ) & \mbox{if } i=j\eqsp,\\
        - \mathrm{softmax}(z)_i \mathrm{softmax}(z)_j & \mbox{otherwise.}
    \end{array}
\right.
$$

\item Write $\ell_{\theta}(X,Y) =  -\sum_{k=1}^{M} \1_{Y=k}\log f_{\theta}(X)_k$ so that 
$$
\ell_n:\theta \mapsto \frac{1}{n} \sum_{i=1}^n \ell_{\theta}(X_i,Y_i)\eqsp. 
$$
Prove that  the gradient with respect to all parameters can be computed as follows.
\begin{align*}
\nabla_{W^L} \ell_{\theta}(X,Y) &= (f_{\theta}(X) - \1_Y)(h_{\theta}^{L-1}(X))^\top\eqsp,\\
\nabla_{b^L} \ell_{\theta}(X,Y) &= f_{\theta}(X) - \1_Y\eqsp,
\end{align*}
where $\1_Y$ is the vector where all entries equal to 0 except the entry with index $Y$ which equals 1.

\item Prove that for all $1\leqslant m\leqslant L-1$,
\begin{align*}
\nabla_{W^m} \ell_{\theta}(X,Y) &= \nabla_{z_{\theta}^m(X)}\ell_{\theta}(X,Y)(h_{\theta}^{m-1}(X))^\top\eqsp,\\
\nabla_{b^m} \ell_{\theta}(X,Y) &=  \nabla_{z_{\theta}^m(X)}\ell_{\theta}(X,Y)\eqsp,
\end{align*}
where $\nabla_{z_{\theta}^m(X)}$ is computed recursively as follows.
\begin{align*}
\nabla_{z^L(X)} \ell_{\theta}(X,Y) &= \ell_{\theta}(X,Y) - \1_Y\eqsp,\\
\nabla_{h_{\theta}^m(X)} \ell_{\theta}(X,Y) &= (W^{m+1})^\top\nabla_{z_{\theta}^{m+1}(X)} \ell_{\theta}(X,Y) \eqsp,\\
\nabla_{z_{\theta}^m(X)} \ell_{\theta}(X,Y) &= \nabla_{h_{\theta}^m(X)}\ell_{\theta}(X,Y) \odot \varphi_m'(z_{\theta}^{m}(X))\eqsp,
\end{align*}
where $\odot$ is the elementwise multiplication.

\end{enumerate}
\end{document} 