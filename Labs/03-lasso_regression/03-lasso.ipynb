{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <font color=darkcyan> Multivariate linear regression - Lasso </font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib as mpl\n",
    "from matplotlib import colors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is assumed that for all $1\\leqslant i \\leqslant n$, \n",
    "\n",
    "$$\n",
    "Y_i = X^\\top_i \\beta_{\\star} + \\varepsilon_i\\,,\n",
    "$$\n",
    "\n",
    "where the $(\\varepsilon_i)_{1\\leqslant i\\leqslant n}$ are i.i.d. random variables in $\\mathbb{R}$, $X_i\\in\\mathbb{R}^d$ and $\\beta_{\\star}$ is an unknown vector in $\\mathbb{R}^d$. Let $Y\\in\\mathbb{R}^n$ (resp. $\\varepsilon\\in\\mathbb{R}^n$)  be the random vector such that  for all $1\\leqslant i \\leqslant n$, the $i$-th component of $Y$ (resp. $\\varepsilon$) is $Y_i$ (resp. $\\varepsilon_i$) and $X\\in\\mathbb{R}^{n\\times d}$ the matrix with line $i$ equal to $X^\\top_i$. The model is then written\n",
    "\n",
    "$$\n",
    "Y = X \\beta_{\\star} + \\varepsilon\\,.\n",
    "$$\n",
    "\n",
    "In this section, it is assumed that $\\mathbb{E}[\\varepsilon] = 0$ and $\\mathbb{E}[\\varepsilon \\varepsilon^\\top] = \\sigma_{\\star}^2 I_n$. The Lasso estimate of $\\beta_{\\star}$ is defined as a solution to\n",
    "\n",
    "$$\n",
    "\\widehat \\beta_n\\in  \\mathrm{argmin}_{\\beta\\in\\mathbb{R}^d}\\,\\left( n^{-1}\\|Y - X\\beta\\|_2^2 + \\lambda \\|\\beta\\|_1\\right)\\,.\n",
    "$$\n",
    "\n",
    "where $\\lambda>0$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=darkred> Explain the coordinate-wise optimization procedure </font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A coordinate descent can be applied to solve the LASSO optimization problem. In this case, solving the Lasso optimization problem amounts to producing iterative estimators, where at each iteration, a coordinate is selected to be updated. Then, the objective function  is optimized explicitly  with respect to the selected coordinate. For all $\\beta \\in \\mathbb{R}^d$,  \n",
    "$$\n",
    "\\nabla_\\beta \\|Y - X\\beta\\|_2^2 = - 2 X^\\top (Y-X\\beta)\\,.\n",
    "$$\n",
    "Then, for all $1\\leqslant j \\leqslant d$, $(\\nabla_\\beta \\|Y - X\\beta\\|_2^2)_j = -2 {\\bf X}^\\top_j (Y-X\\beta)$, where ${\\bf X}_j$ is the $j$-th column of the matrix $X$. \n",
    "Define, for all $1\\leqslant j \\leqslant d$,\n",
    "$$\n",
    "\\upsilon_{j}={\\bf X}^\\top_{j}\\left(Y-\\sum_{\\substack{i=1\\\\ i\\neq j}}^d\\beta_{i}{\\bf X}_{i}\\right)\\,.\n",
    "$$\n",
    "Assuming that the columns of $X$ are normalized, i.e. for all $1\\leqslant k \\leqslant d$, ${\\bf X}^\\top_{k}{\\bf X}_{k}=1$, yields\n",
    "$$\n",
    "(\\nabla_\\beta \\|Y - X\\beta\\|_2^2)_j = -2( \\upsilon_j - \\beta_j)\\,.\n",
    "$$\n",
    "Consequently, for all $\\beta_j \\neq 0$, \n",
    "$$\n",
    "(\\nabla_\\beta ( n^{-1}\\|Y - X\\beta\\|_2^2 +  \\lambda\\|\\beta\\|_1))_j= \\frac{2}{n}( \\beta_j - \\upsilon_j + \\lambda n\\textrm{sign}(\\beta_j)/2)\\,.\n",
    "$$\n",
    "For all $1\\leqslant j\\leqslant d$,  $\\beta_j \\mapsto  n^{-1}\\|Y - X\\beta\\|_2^2 + \\lambda\\|\\beta\\|_1$ is convex and grows to infinity when $|\\beta_j|\\to \\infty$ and admits thus a minimum at some $\\beta_j^{\\star}\\in\\mathbb{R}$. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- If $\\beta_j^{\\star} \\neq 0$, then\n",
    "$$\n",
    "\\beta_j^{\\star} = \\upsilon_j\\left( 1 - \\frac{\\lambda n~\\textrm{sign}(\\beta_j^{\\star})}{2 \\upsilon_j}\\right)\\,,\n",
    "$$\n",
    "which yields, as  $\\textrm{sign}(\\beta_j^{\\star}) = \\textrm{sign}(\\upsilon_j)$,\n",
    "$$\n",
    "\\beta_j^{\\star} = \\upsilon_j\\left(1 - \\frac{\\lambda n }{2 |\\upsilon_j|}\\right)\n",
    "$$\n",
    "and\n",
    "$$\n",
    "1 - \\frac{\\lambda n }{2 |\\upsilon_j|} \\geqslant 0\\,.\n",
    "$$\n",
    "- If $1 - \\lambda n/(2 |\\upsilon_j|)<0$, there is no solution to $(\\nabla_\\beta ( n^{-1}\\|Y - X\\beta\\|_2^2 +  \\lambda\\|\\beta\\|_1))_j=0$ for $\\beta_j \\neq 0$.  Since $\\beta_j \\mapsto  n^{-1}\\|Y - X\\beta\\|_2^2 + \\lambda\\|\\beta\\|_1$ admits a minimum, $\\beta_j^{\\star}=0$. \n",
    "Therefore,\n",
    "$$\n",
    "\\beta_j^{\\star} = \\upsilon_j\\left( 1 - \\frac{\\lambda n}{2 |\\upsilon_j|}\\right)_+ = \\mathrm{max}\\left(0;\\upsilon_j\\left( 1 - \\frac{\\lambda n}{2 |\\upsilon_j|}\\right)\\right)\\,.\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As a conlusion:\n",
    "- if $|\\upsilon_j|\\geq \\lambda n / 2$, $\\beta_j^{\\star} = \\upsilon_j( 1 - \\frac{\\lambda n}{2 |\\upsilon_j|})$ and if $|\\upsilon_j|< \\lambda n / 2$, $\\beta_j^{\\star} = 0$ ;\n",
    "- if $\\upsilon_j\\leq -\\lambda n / 2$, $\\beta_j^{\\star} = \\upsilon_j + \\frac{\\lambda n}{2}$,  if $\\upsilon_j\\geq \\lambda n / 2$, $\\beta_j^{\\star} = \\upsilon_j - \\frac{\\lambda n}{2}$, and if $|\\upsilon_j|< \\lambda n / 2$, $\\beta_j^{\\star} = 0$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Import data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data frames can be imported using pandas. This provides two-dimensional and heterogeneous tabular data.\n",
    "https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=darkred>\n",
    "Import data in the file BRinf using ``read_csv``, display the first rows with ``head`` and the shape of the dataframe using ``shape``.\n",
    "</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# In this section, multivariate linear regression is used to predic the Brazilian inflation based on\n",
    "# many observed variables, see https://github.com/gabrielrvsc/HDeconometrics/\n",
    "df = pd.read_csv('BRinf.txt')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# number of observations, number of variables\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=darkred>\n",
    "Use the ``StandardScaler`` of sklearn to preprocess the input variables.\n",
    "</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "``StandardScaler`` standardizes the input variables by removing the mean and scaling to unit variance.\n",
    "We will not analyze closely standardization in this course. However, it is often very useful (even mandatory in some cases) for the stability of learning procedures.\n",
    "https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.StandardScaler.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "sc = StandardScaler()\n",
    "X = sc.fit_transform(df.iloc[:,2:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# first coordinate is the number of samples\n",
    "# second coordinate is the number of input features (+ 1 for the observations)\n",
    "np.shape(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=darkred>\n",
    "Build two datasets. \n",
    "    ``X_train`` and ``Y_train`` contain the first 140 input data and observations. ``X_test`` and ``Y_test`` contain the remaining input data and observations. We train a linear regression model using ``X_train`` and ``Y_train`` and we assess the performance of the model using ``X_test`` and ``Y_test``. \n",
    "</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://pandas.pydata.org/docs/reference/frame.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nb_data_train = 140\n",
    "# inflation observations\n",
    "Y_train = df.iloc[:nb_data_train,1] \n",
    "Y_test  = df.iloc[nb_data_train:,1] \n",
    "Y_test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# other variables\n",
    "X_train = X[:nb_data_train,:] \n",
    "X_test  = X[nb_data_train:,:] \n",
    "np.shape(X_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Lasso Regression from scractch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=darkred>\n",
    "Write a ``threshold_function`` function with arguments a real number ``z`` and a positive number ``$\\alpha$`` which returns \n",
    "    \n",
    "    - $z+\\alpha$ if $z<-\\alpha$,\n",
    "    \n",
    "    - $z-\\alpha$ if $z>\\alpha$,\n",
    "    \n",
    "    - 0 otherwise.\n",
    "</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def threshold_func(z,alpha):\n",
    "    if z < - alpha:\n",
    "        return (z + alpha)\n",
    "    elif z >  alpha:\n",
    "        return (z - alpha)\n",
    "    else: \n",
    "        return 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=darkred>\n",
    "    Write a ``coordinate_descent_lasso`` function with arguments an initial estimate ``$\\beta$``, the data ``X`` and ``y``, a penalty parameter ``$\\alpha$`` and a number of iterations ``n_iter``. The function returns the parameter estimate after n_iter iterations of the coordinate-wise optimization procedure.\n",
    "    </font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To make things simpler, you can write the function with $\\alpha = \\lambda n /2$ in the mathematical derivation above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def coordinate_descent_lasso(beta,X,y,alpha, n_iter):\n",
    "    n,d = X.shape\n",
    "    X = X / (np.linalg.norm(X,axis = 0)) \n",
    "    for i in range(num_iter): \n",
    "        for j in range(d):\n",
    "            X_j = X[:,j].reshape(-1,1)\n",
    "            y_pred = X @ beta\n",
    "            r_j = X_j.T @ (y - y_pred  + beta[j]*X_j)\n",
    "            beta[j] =  threshold_func(r_j, alpha)         \n",
    "    return beta"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=darkred>\n",
    "    Run the algorithm with several values of $\\alpha$ using X_train and Y_train and display the number of zero coeeficients of the parameter estimate and the MSE obtained on the test set.\n",
    "    </font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Lasso Regression with Sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import Lasso, LassoCV, Ridge, RidgeCV"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=darkred>\n",
    "Create a np array with several values of the penalty parameter (called $\\alpha$ in Python)\n",
    "</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_alphas = 100\n",
    "alphas = np.logspace(-10, -1, n_alphas)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=darkred>\n",
    "Use the ``fit`` function of sklearn to fit a Lasso model with for each value of $\\alpha$. \n",
    "    \n",
    "Store the estimated parameter, the number of zeros in the estimated parameter and the MSE on the test set after each training.\n",
    "</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lasso = Lasso(max_iter=10000, normalize=True)\n",
    "coefs = []\n",
    "nb_zeros = []\n",
    "rmse = []\n",
    "for a in alphas:\n",
    "    lasso.set_params(alpha=a)\n",
    "    lasso.fit(X_train, Y_train)\n",
    "    coefs.append(lasso.coef_)\n",
    "    nb_zeros.append(np.count_nonzero(lasso.coef_==0))\n",
    "    y_hat = lasso.predict(X_test)\n",
    "    rmse.append(mean_squared_error(y_hat,Y_test.values))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=darkred>\n",
    "Display the estimated parameters as a function of the penalty parameter.\n",
    "</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(1,figsize=(8,8))\n",
    "ax = plt.gca()\n",
    "ax.plot(alphas, coefs)\n",
    "ax.set_xscale(\"log\")\n",
    "ax.set_xlim(ax.get_xlim()[::-1])  # reverse axis\n",
    "plt.xlabel(\"lambda\")\n",
    "plt.ylabel(r'$\\widehat\\theta^{\\mathrm{lasso}}_{n,\\lambda}$')\n",
    "plt.title(\"Lasso coefficients as a function of the regularization\")\n",
    "plt.axis(\"tight\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=darkred>\n",
    "Display the number of zero coefficients of the estimated parameter as a function of the penalty parameter.\n",
    "</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(1,figsize=(8,8))\n",
    "ax = plt.gca()\n",
    "ax.plot(alphas, nb_zeros)\n",
    "ax.set_xscale(\"log\")\n",
    "ax.set_xlim(ax.get_xlim()[::-1])  # reverse axis\n",
    "plt.xlabel(\"lambda\")\n",
    "plt.ylabel('Number of zero coefficients')\n",
    "plt.title(\"Number of null coefficients as a function of the regularization\")\n",
    "plt.axis(\"tight\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=darkred>\n",
    "Display the MSE on the test set as a function of the penalty parameter.\n",
    "</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(1,figsize=(8,8))\n",
    "ax = plt.gca()\n",
    "ax.plot(alphas, rmse)\n",
    "ax.set_xscale(\"log\")\n",
    "ax.set_xlim(ax.get_xlim()[::-1])  # reverse axis\n",
    "plt.xlabel(\"lambda\")\n",
    "plt.ylabel('Mean squared error')\n",
    "plt.title(\"Mean squared error as a function of the regularization\")\n",
    "plt.axis(\"tight\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
