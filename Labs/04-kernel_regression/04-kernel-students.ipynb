{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <font color=darkcyan> Multivariate linear regression - Kernel regression </font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib as mpl\n",
    "from matplotlib import colors \n",
    "from sklearn.metrics import mean_squared_error"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A function $k:\\mathbb{R}^d\\times\\mathbb{R}^d:\\to \\mathbb{R}$ is said to be a positive semi-definite kernel if and only if it is symmetric and if for all $n\\geqslant 1$, $(x_1,\\ldots,x_n)\\in(\\mathbb{R}^d)^n$ and all $(a_1,\\ldots,a_n)\\in\\mathbb{R}^n$,\n",
    "$$\n",
    "\\sum_{1\\leqslant i,j\\leqslant n}a_ia_jk(x_i,x_j) \\geqslant 0\\,.\n",
    "$$\n",
    "\n",
    "The following functions, defined on $\\mathbb{R}^d\\times\\mathbb{R}^d$, are positive semi-definite kernels:\n",
    "\n",
    "$$\n",
    "k:(x,y)\\mapsto x^Ty \\quad\\mathrm{and}\\quad k:(x,y)\\mapsto \\mathrm{exp}\\left(-\\|x-y\\|^2/(2\\sigma^2\\right)\\,,\\; \\sigma>0\\,.\n",
    "$$\n",
    "\n",
    "\n",
    "Let $\\mathcal{F}$ be a Hilbert space of functions $f:\\mathbb{R}^d\\to\\mathbb{R}$. A symmetric function $k:\\mathbb{R}^d\\times\\mathbb{R}^d:\\to \\mathbb{R}$ is said to be a reproducing kernel of $\\mathcal{F}$ if and only if:\n",
    "\n",
    "1. for all $x\\in\\mathbb{R}^d$, $k(x,\\cdot)\\in\\mathcal{F}$ ; \n",
    "\n",
    "2. for all $x\\in\\mathbb{R}^d$ and all $f\\in\\mathcal{F}$, $\\langle f; k(x,\\cdot)\\rangle_\\mathcal{F} = f(x)$ . \n",
    "\n",
    "The space $\\mathcal{F}$ is said to be a reproducing kernel Hilbert space with kernel $k$.\n",
    "\n",
    "\n",
    "\n",
    "Let $k:\\mathbb{R}^d\\times\\mathbb{R}^d:\\to \\mathbb{R}$ be a positive definite kernel and $\\mathcal{F}$ the RKHS with kernel $k$. Then, \n",
    "\n",
    "$$\n",
    "\\widehat f^n_{\\mathcal{F}} \\in \\underset{f\\in\\mathcal{F}}{\\mathrm{min}}\\;\\frac{1}{n}\\sum_{i=1}^n (Y_i - f(X_i))^2 + \\lambda\\|f\\|_\\mathcal{F}^2\\,,\n",
    "$$\n",
    "\n",
    "where $\\|f\\|^2_\\mathcal{F} = \\langle f\\,;\\, f\\rangle_\\mathcal{F}$, is given by $\\widehat f^n_{\\mathcal{F}} : x \\mapsto \\sum_{i=1}^n \\widehat \\alpha_i k(X_i,x)$, where\n",
    "\n",
    "$$\n",
    "\\widehat\\alpha \\in \\underset{\\alpha \\in (\\mathbb{R}^d)^n}{\\mathrm{argmin}}\\;\\left\\{\\frac{1}{n}\\|Y - K\\alpha\\|^2_2 + \\lambda \\sum_{1\\leqslant i,j \\leqslant n}\\alpha_i \\alpha_j k(X_i,X_j) = \\frac{1}{n}\\|Y - K\\alpha\\|^2_2 + \\lambda \\alpha^TK\\alpha\\right\\}\\,,\n",
    "$$\n",
    "\n",
    "where for all $1\\leqslant i,j\\leqslant n$, $K_{i,j} = k(X_i,X_j)$.\n",
    "\n",
    "In practice, once the matrix $K$ is built, kernel ridge regression boils down to solving this optimization problem to obtain $\\widehat \\alpha$. Then, the estimated function $\\widehat f^n_{\\mathcal{F}}$ is a mixture of kernels evaluated at each data points with weights given by $\\alpha$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=darkred> Provide the value of $\\widehat \\alpha$ as a function of $n$, $\\lambda$, $K$ and  $Y$.</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Import data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data frames can be imported using pandas. This provides two-dimensional and heterogeneous tabular data.\n",
    "https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=darkred>\n",
    "    \n",
    "Import data in the file BRinf using ``read_csv``, display the first rows with ``head`` and the shape of the dataframe using ``shape``.\n",
    "</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# In this section, multivariate linear regression is used to predic the Brazilian inflation based on\n",
    "# many observed variables, see https://github.com/gabrielrvsc/HDeconometrics/\n",
    "df = pd.read_csv('BRinf.txt')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# number of observations, number of variables\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df.iloc[:,2:]\n",
    "Y = df.iloc[:,1] "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=darkred>\n",
    "    \n",
    "Use the ``StandardScaler`` of sklearn to preprocess the input variables.\n",
    "</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "``StandardScaler`` standardizes the input variables by removing the mean and scaling to unit variance.\n",
    "We will not analyze closely standardization in this course. However, it is often very useful (even mandatory in some cases) for the stability of learning procedures.\n",
    "https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.StandardScaler.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "sc = StandardScaler()\n",
    "X = sc.fit_transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# first coordinate is the number of samples\n",
    "# second coordinate is the number of input features (+ 1 for the observations)\n",
    "np.shape(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=darkred>\n",
    "    \n",
    "Divide the input data ``X`` and observations ``Y`` into training and test sets using the ``train_test_split`` method. This method randomly splits arrays or matrices into training and test subsets. It allows to train several times a model with different training set and analyze the variability of the performance on the test set.\n",
    "</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.1)\n",
    "np.shape(X_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Kernel Regression from scractch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=darkred>\n",
    "\n",
    "Write a ``linear_kernel`` function with the arguments two vectors ``x`` and ``y``, which returns the result of the linear kernel function defined as $k:(x,y)\\mapsto x^Ty$.\n",
    "</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=darkred>\n",
    "\n",
    "Write a ``rbf_kernel`` function with the arguments two vectors ``x`` and ``y`` and a scalar ``sigma``, which returns the result of the radial basis function (RBF) kernel defined as $k:(x,y)\\mapsto \\mathrm{exp}\\left(-\\|x-y\\|^2/(2\\sigma^2\\right)$.\n",
    "</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=darkred>\n",
    "\n",
    "Write a ``kernel_ridge_regression`` function with arguments the data ``X`` and ``y``, the kernel matrix ``K`` and a penalty parameter ``$\\lambda$``. The function returns the parameter estimate $\\widehat \\alpha$.\n",
    "</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=darkred>\n",
    "    Run the algorithm with several values of $\\lambda$ using X_train and Y_train and compute MSE obtained on the test set.\n",
    "    </font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Kernel Regression with Sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.kernel_ridge import KernelRidge"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=darkred>\n",
    "    \n",
    "Fit a ``KernelRidge`` from sklearn to train a kernel regression model, display the parameter estimate $\\widehat \\alpha$ and compute the MSE obtained on the test set.\n",
    "</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Cross Validation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=darkred>\n",
    "    \n",
    "Implement a grid search from scratch for the given ``KernelRidge`` model with radial basis function (RBF) kernel and hyperparameter ``alpha``. Create a new training dataset (X_train, Y_train) and a validation dataset (X_val, Y_val) using the actual (X_train, Y_train) datasets. Write a function that performs grid search to find the best value of ``alpha`` and displays the best hyperparameter along with the corresponding validation MSE.\n",
    "</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = KernelRidge(kernel='rbf', gamma=0.1)\n",
    "\n",
    "# Validation set\n",
    "X_train, X_val, Y_train, Y_val = train_test_split(X_train, Y_train, test_size=0.1)\n",
    "\n",
    "\n",
    "# Hyperparameter grid for alpha\n",
    "alphas = [1e0, 0.1, 1e-2, 1e-3]\n",
    "\n",
    "best_alpha = None\n",
    "best_score = float('inf')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=darkred>\n",
    "    \n",
    "Implement a grid search using either ``GridSearchCV`` or ``RandomizedSearchCV`` for the given ``KernelRidge`` model with a radial basis function (RBF) kernel and hyperparameters ``alpha`` and ``gamma``. Display the optimal hyperparameters along with the corresponding validation MSE.\n",
    "    \n",
    "</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV, RandomizedSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = KernelRidge(kernel='rbf', gamma=0.1)\n",
    "param_grid={\"alpha\": [1e0, 0.1, 1e-2, 1e-3],\n",
    "            \"gamma\": np.logspace(-2, 2, 5)}\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Ridge vs Lasso"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import Lasso, Ridge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "from sklearn.exceptions import ConvergenceWarning\n",
    "\n",
    "# Ignore ConvergenceWarning\n",
    "warnings.filterwarnings(\"ignore\", category=ConvergenceWarning)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=darkred>\n",
    "Create a np array with several values of the penalty parameter (called $\\alpha$ in Python)\n",
    "</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_alphas = 100\n",
    "alphas = np.logspace(-10, 3, n_alphas)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=darkred>\n",
    "Use the ``fit`` function of sklearn to fit a Lasso and Ridge model with for each value of $\\alpha$. \n",
    "    \n",
    "Store the estimated parameter, the number of zeros in the estimated parameter and the MSE on the test set after each training.\n",
    "</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=darkred>\n",
    "Display the estimated parameters as a function of the penalty parameter.\n",
    "</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=darkred>\n",
    "Display the number of zero coefficients of the estimated parameter as a function of the penalty parameter.\n",
    "</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=darkred>\n",
    "Display the MSE on the test set as a function of the penalty parameter.\n",
    "</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
