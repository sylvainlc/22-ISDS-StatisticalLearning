{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <font color=darkcyan> Logistic regression</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ignore warnings for better clarity (may not be the best thing to do)...\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Required packages\n",
    "np.set_printoptions(precision=2) \n",
    "from numpy.random import multivariate_normal\n",
    "from scipy.linalg.special_matrices import toeplitz\n",
    "from numpy.random import randn\n",
    "from numpy.linalg import norm\n",
    "from scipy.optimize import check_grad\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font color=darkred>  Model </font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\rightharpoondown$ The objective is to predict the  label $Y\\in\\{0,1\\}$ based on $X\\in\\mathbb{R}^d$.\n",
    "\n",
    "$\\rightharpoondown$ Logistic regression models the distribution of $Y$ given $X$.\n",
    "\n",
    "\\begin{equation*}\n",
    "\\mathbb{P}(Y = 1| X) = \\sigma(\\langle w,X \\rangle)\\,,\n",
    "\\end{equation*}\n",
    "where $w \\in \\mathbb{R}^d$ is a vector of model weights, and where $\\sigma$ is the sigmoid function.\n",
    "\n",
    "$$\n",
    "\\sigma: z \\mapsto \\frac{1}{1 + e^{-z}}\\,.\n",
    "$$\n",
    "\n",
    "$\\rightharpoondown$ The sigmoid function is a \\alert{model choice to map $\\mathbb{R}$ into $(0,1)$}."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(x):\n",
    "    expx = np.exp(x)\n",
    "    z   = expx / (1. + expx)\n",
    "    return z\n",
    "\n",
    "def sample_logistic(w0, n_samples=1000, corr=0.5):\n",
    "    n_features = w0.shape[0]\n",
    "    cov        = toeplitz(corr ** np.arange(0, n_features))\n",
    "    X          = multivariate_normal(np.zeros(n_features), cov, size=n_samples)\n",
    "    p          = sigmoid(X.dot(w0))\n",
    "    y          = np.random.binomial(1, p, size=n_samples)\n",
    "    return X, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.linspace(-8,8,100)\n",
    "z = sigmoid(x)\n",
    "plt.plot(x,z,linestyle = \"dashed\",color=\"blue\", label= \"Sigmoid function\")\n",
    "plt.plot(x,.5*np.ones(np.size(x)),linestyle = \"dashed\",color=\"red\", label=\"Threshold 1/2\")\n",
    "plt.ylabel('Probability of  the event {Y=1}', fontsize=12)\n",
    "plt.xlabel('x', fontsize=12)\n",
    "plt.tick_params(labelright=True)\n",
    "plt.grid('True')\n",
    "plt.legend();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Logistic regression models the distribution of $Y$ given $X$.\n",
    "\n",
    "\\begin{equation*}\n",
    "\\mathbb{P}(Y = 1| X) = \\sigma(\\langle w,X \\rangle)\\,,\n",
    "\\end{equation*}\n",
    "\n",
    "The graph above illustrates that the Bayes classification rule in this case is\n",
    "$f^*(X) = 1$ if and only if $\\langle w,X \\rangle>0$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_samples  = 1000\n",
    "n_features = 2\n",
    "\n",
    "w0   = multivariate_normal([-2,1], np.eye(2))\n",
    "\n",
    "X, y = sample_logistic(w0, n_samples=n_samples, corr = 0.3)\n",
    "\n",
    "simulated_data          = pd.DataFrame(columns = [\"x\",\"y\",\"Label\"])\n",
    "simulated_data[\"x1\"]     = X[:,0]\n",
    "simulated_data[\"x2\"]     = X[:,1]\n",
    "simulated_data[\"Label\"] = y\n",
    "\n",
    "sns.set_style(\"whitegrid\")\n",
    "sns.lmplot(x = \"x1\", y = \"x2\", data = simulated_data, fit_reg = False, hue = \"Label\", legend = True, scatter_kws={\"s\": 5})\n",
    "\n",
    "plt.title(\"Logistic regression simulation\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def decision_function(X,w):\n",
    "    z = sigmoid(X.dot(w))\n",
    "    return z\n",
    "    \n",
    "xlim  = [np.min(X[:,0]), np.max(X[:,0])]\n",
    "ylim  = [np.min(X[:,1]), np.max(X[:,1])]\n",
    "xplot = np.linspace(xlim[0], xlim[1], 30)\n",
    "yplot = np.linspace(ylim[0], ylim[1], 30)\n",
    "\n",
    "Yplot, Xplot = np.meshgrid(yplot, xplot)\n",
    "xy           = np.vstack([Xplot.ravel(), Yplot.ravel()]).T\n",
    "P            = decision_function(xy,w0).reshape(Xplot.shape)\n",
    "\n",
    "sns.set_style(\"whitegrid\")\n",
    "sns.lmplot(x = \"x1\", y = \"x2\", data = simulated_data, fit_reg = False, hue = \"Label\", legend = True, scatter_kws={\"s\": 5})\n",
    "\n",
    "plt.title(\"Logistic regression simulation\");\n",
    "\n",
    "# plot decision boundary and margins\n",
    "plt.contour(Xplot, Yplot, P, colors = 'k', levels = [0.5], alpha = 0.8, linestyles = ['--']);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font color=darkred>  Logistic regression: losses and gradients </font>\n",
    "\n",
    "The aim of this section is to detail how to solve the following optimization problem\n",
    "$$\n",
    "\\arg\\min_{w \\in \\mathbb R^d} \\Big\\{ f(w) + \\frac{\\lambda}{2} \\|w\\|_2^2 \\Big\\}\\,,\n",
    "$$\n",
    "where $d$ is the number of features.\n",
    "\n",
    "$$\n",
    "f: w \\mapsto \\frac 1n \\sum_{i=1}^n f_i(w) = \\frac{1}{n} \\sum_{i=1}^n \\{-y_ix^\\top_iw + \\log(1 + \\exp(x_i^\\top w))\\} + \\frac{\\lambda}{2} \\|w\\|_2^2\\,,\n",
    "$$\n",
    "where $n$ is the sample size, and where $y_i \\in \\{ 0, 1 \\}$ for all $1\\leqslant i\\leqslant n$.\n",
    " \n",
    "A basic gradient descent algorithm requires to compute the functions $f$ and $\\nabla f$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=darkred>  Write the likelihood of the logistic regression model and its gradient. You can build a class Logistic Regression which allows to compute the loss function and its gradient.</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LogisticRegression:\n",
    "    def __init__(self, X, y, lmbd):\n",
    "        self.X = X\n",
    "        self.y = y\n",
    "        self.lmbd = lmbd\n",
    "        self.n_samples, self.n_features = X.shape\n",
    "    \n",
    "    def loss(self, w):\n",
    "        # Computes f(w)\n",
    "        y, X, n_samples, lmbd = self.y, self.X, self.n_samples, self.lmbd\n",
    "        res = 0\n",
    "        \n",
    "    \n",
    "    def grad(self, w):\n",
    "        # Computes the gradient of f at w\n",
    "        y, X, n_samples, lmbd = self.y, self.X, self.n_samples, self.lmbd\n",
    "        res = 0\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check numerically the gradient using the function checkgrad from scipy.optimize\n",
    "# Use the function simu_logreg to simulate data according to the logistic regression model\n",
    "n_features = 10\n",
    "w_true     = np.random.randn(n_features)\n",
    "X, y       = sample_logistic(w_true, n_samples, corr=0.1)\n",
    "model      = LogisticRegression(X, y, 1e-3)\n",
    "# check_grad assesses the correctness of a gradient by comparing it to a finite-difference approximation\n",
    "check_grad(model.loss, model.grad, w_true)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font color=darkred> Gradient descent </font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the case of machine/deep learning applications, the function to be minimized is of the form:\n",
    "$$\n",
    "f:w\\mapsto  \\frac{1}{n}\\sum_{i=1}^n \\ell(Y_i, \\langle w; X_i \\rangle) + \\lambda g(w) = \\frac{1}{n}\\sum_{i=1}^n f_i(w)\\,.\n",
    "$$\n",
    "The most simple method  is based on full gradients, since at each iteration  it requires to compute\n",
    "$$\n",
    "\\nabla f(w) = \\frac 1n \\sum_{i=1}^n \\nabla  f_i(w)\\,,\n",
    "$$\n",
    "which depends on the whole dataset. When processing very large datasets ($n$ is large), this approach has a highly prohibitive computational cost  for a  unique step towards the minimum. \n",
    "For all $k\\geqslant 1$, set\n",
    "$$\n",
    "w^{(k)} = w^{(k-1)} - \\eta_k \\nabla f_{}(w^{(k-1)})\\,.\n",
    "$$\n",
    "Each iteration has complexity $O(nd)$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=darkred>  Write a gradient descent function with inputs: a logistic regression model, an initial estimate, and a maximum number of iterations and a step-size.</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=darkred>  Run the algorithm and display the loss along iterations. Display the impact of the choice of the stepsize.</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=darkred>  Adapt the previous gradient descent algorithm to design a stochastic gradient descent with inputs: a logistic regression model, an initial estimate, and a maximum number of iterations, a constant c and a rate alpha (the stepsize at iteration $k$ is $\\mathrm{c}/k^\\alpha$).</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font color=darkred> Roc curve and auc</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_test = int(0.2*n_samples)\n",
    "X_test, y_test = sample_logistic(w_true, n_samples = n_test, corr = 0.8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# predicted class using the estimated model\n",
    "y_pred = y_test.copy()\n",
    "for i in range(n_test):\n",
    "    if X_test[i].dot(w)>0:\n",
    "        y_pred[i] = 1.0\n",
    "    else:\n",
    "        y_pred[i] = 0.0\n",
    "print('Mean prediction error with the estimated parameter: %f'%np.mean(np.abs(y_pred-y_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# predicted probability for each new individual using the estimated model\n",
    "y_score = np.zeros(n_test)\n",
    "for i in range(n_test):\n",
    "    y_score[i] = sigmoid(X_test[i].dot(w))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Receiver Operating Characteristic (ROC) curve assesses the diagnostic ability of the classifier  as the classification threshold is modified. Logistic regression models the distribution of $Y$ given $X$ as\n",
    "\n",
    "\\begin{equation*}\n",
    "\\mathbb{P}(Y = 1| X) = \\sigma(\\langle w,X \\rangle + b)\\,,\n",
    "\\end{equation*}\n",
    "\n",
    "and the Bayes classifier is defined as \n",
    "\n",
    "$f^*(X) = 1$ if and only if $\\mathbb{P}(Y = 1| X) > \\mathbb{P}(Y = 0| X)$,\n",
    "\n",
    "which is equivalent to \n",
    "\n",
    "$f^*(X) = 1$ if and only if $\\langle w,X \\rangle + b>0$,\n",
    "\n",
    "or also to \n",
    "\n",
    "$f^*(X) = 1$ if and only if $\\mathbb{P}(Y = 1| X) >1/2$. \n",
    "\n",
    "Therefore, the theoretical threshold to classify individuals is $1/2$. However, analyzing the sensitivity of the classifier to this threshold may be interesting which is the aim of the ROC curve which displays the True positive rate as a function of the False positive rate when the threshold is changed. For each value $p^*\\in(0,1)$ the ROC curve classifies individuals using\n",
    "\n",
    "$f^*(X) = 1$ if and only if $\\mathbb{P}(Y = 1| X) > p^*$\n",
    "\n",
    "and plots the True positive rate as a function of the False positive rate. \n",
    "\n",
    "Depending on the application, an optimal threshold may then be used to obtain satisfying True and False positive rates on the test data set."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=darkred>  Use the roc_curve metric from sklearn to display the ROC curve of your logistic regression model.</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font color=darkred> Softmax regression </font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Logistic regression can be extended to classify data in more than two groups. Softmax regression provides a model for the probability that an input $x$ is associated with each group.  It is assumed that the probability to belong to the class $k\\in\\{1,\\ldots,M\\}$ can be expressed by \n",
    "\\begin{equation*}\n",
    "\\mathbb{P}(Y = k| X) = \\frac{\\exp(\\langle w_k,X \\rangle + b_k)}{\\sum_{\\ell=1}^{M}\\exp(\\langle w_\\ell,X \\rangle + b_\\ell)} = p_k(X)\\,,\n",
    "\\end{equation*}\n",
    "where $w_\\ell \\in \\mathbb{R}^d$ and $b_\\ell$  are model \\textbf{weights} and \\textbf{intercepts} for each class.\n",
    "\n",
    "\n",
    "To estimate these unknown parameters, a maximum likelihood approach is used as in the logistic regression setting. In this case, the loss function is given by the negative log-likelihood."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
