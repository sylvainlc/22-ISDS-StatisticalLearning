{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CF7Q8jEeEPM1"
   },
   "source": [
    "## <font color=darkblue> Introduction to Feed Forward Neural Networks</font>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NG6cr03iSYt6"
   },
   "source": [
    "In this notebook, we provide an **overview of feed forward neural networks**, also known as Multi Layer Perceptrons (MLP) or Dense Neural Networks, which are commonly used in machine/deep learning.\n",
    "These networks are very flexible to propose nonlinear models for regression and classification tasks.\n",
    "\n",
    "In this session we set the focus on these simple networks which allows us to discuss backpropagation of gradient and implementation in keras/torch.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gv1tTxcDjLW6"
   },
   "source": [
    "### <font color=darkred> Bibliography & additional ressources </font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SLsK5IHEjPWb"
   },
   "source": [
    "-  [1] Probabilistic machine learning: an introduction, Kevin P. Murphy, 2022, https://probml.github.io/pml-book/book1.html\n",
    "``Full book online with all basics on machine learning. Not state-of-the-art but very good introduction``\n",
    "- [2] Deep learning course, Ms. Sc. Institut Polytechnique de Paris, https://github.com/m2dsupsdlclass/lectures-labs\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LknAWbm6z_gR"
   },
   "source": [
    "### <font color=darkred>Introduction : general framework & motivations</font>\n",
    "\n",
    "Parameter inference in statistical/machine learning often boils down to solving\n",
    "\n",
    "\\begin{equation*}\n",
    "\\mathrm{argmin}_{\\theta \\in \\mathbb{R}^d} \\,\\{\\ell_n(\\theta) + \\lambda \\mathrm{pen}(\\theta)\\}\\,,\n",
    "\\end{equation*}\n",
    "\n",
    "with $\\lambda>0$, $\\mathrm{pen}(\\cdot)$ some penalization function and $f$ a ``goodness-of-fit function``  based on a loss $\\ell$,\n",
    "\n",
    "\\begin{equation*}\n",
    "\\ell_n(\\theta) = \\frac 1n \\sum_{i=1}^n \\ell(\\theta,y_i, x_i)\\,,\n",
    "\\end{equation*}\n",
    "\n",
    " where $(x_i,y_i)_{1\\leq i\\leq n}$ are ``training examples of inputs and outputs`` (in a supervised setting), and $w$ is an ``unknown parameter to be estimated.``"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "B61KWCTMHjsy"
   },
   "source": [
    "\n",
    "**Classification**. For instance, in a classification task where $Y_i\\in\\{1,\\ldots, M\\}$ and $X_i\\in\\mathbb{R}^d$, the loglikelihood of the data $\\{(X_i,Y_i)\\}_{1\\leq i\\leq n}$ writes:\n",
    "$$\n",
    "\\log \\mathbb{P}_\\theta(Y_{1:n} = y_{1:n}| X_{1:n}) = \\sum_{i=1}^n \\log \\mathbb{P}_\\theta(Y_{i} = y_i| X_{i})\\,,\n",
    "$$\n",
    "so that\n",
    "$$\n",
    "f:\\theta\\mapsto -\\frac{1}{n}\\sum_{i=1}^n \\log \\mathbb{P}_\\theta(Y_{i} = y_i| X_{i})\\,.\n",
    "$$\n",
    "The only remaining step before training a model and validating the results is to chose a **specific form** for $\\mathbb{P}_\\theta(Y_{i} = y_i| X_{i})$, $1\\leq i \\leq n$.  When $Y_i\\in\\{0,1\\}$, the **logistic regression** models the distribution of $Y$ given $X$ as\n",
    "\\begin{equation*}\n",
    "\\mathbb{P}_\\theta(Y = 1| X) = \\sigma(\\langle \\theta,X \\rangle )\\,,\n",
    "\\end{equation*}\n",
    "where $\\theta \\in \\mathbb{R}^d$ is a vector of model weights, and where $\\sigma$ is the sigmoid function.\n",
    "\n",
    "$$\n",
    "\\sigma: z \\mapsto \\frac{1}{1 + e^{-z}}\\,.\n",
    "$$\n",
    "\n",
    "The common choice when $Y_i\\in\\{1,\\ldots,M\\}$ is the **softmax function**: for all $1\\leqslant i\\leqslant M$\n",
    "$$\n",
    "\\mathbb{P}_{\\theta}(Y_i = k | X_i) = \\frac{\\mathrm{e}^{\\omega_k^TX_i + b_k}}{\\sum_{j=1}^M\\mathrm{e}^{\\omega_j^TX_i + b_j}}\\,.\n",
    "$$\n",
    "The unknown parameters are $\\theta = \\{\\omega_1,b_1,\\ldots,\\omega_M,b_L\\}$.\n",
    "\n",
    "The standard approach to estimate the parameters is by maximizing the logarithm of the likelihood i.e. by minimizing the opposite of the normalized loglikelihood:\n",
    "$$\n",
    "\\ell_n:\\theta \\mapsto -\\frac{1}{n} \\sum_{i=1}^n\\sum_{k=1}^{M} \\mathbb{1}_{Y_i=k}\\log \\mathbb{P}_{\\theta}(Y_i = k | X_i) = -\\frac{1}{n} \\sum_{i=1}^n\\sum_{k=1}^{M} \\mathbb{1}_{Y_i=k}\\log \\frac{\\mathrm{e}^{\\omega_k^TX_i + b_k}}{\\sum_{j=1}^M\\mathrm{e}^{\\omega_j^TX_i + b_j}}\\,.\n",
    "$$\n",
    "This can also be written\n",
    "$$\n",
    "\\ell_n:\\theta \\mapsto -\\frac{1}{n} \\sum_{i=1}^n\\sum_{k=1}^{M} \\mathbb{1}_{Y_i=k}\\left(\\omega_k^TX_i + b_k - \\log \\left(\\sum_{j=1}^M\\mathrm{e}^{\\omega_j^TX_i + b_j}\\right)\\right)\\,.\n",
    "$$\n",
    "Neural networks offer flexible solutions to introduce more complex nonlinear expressions for $\\mathbb{P}_{\\theta}(Y_i = k | X_i)$, $1\\leq k \\leq M$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hxPIual20G-B"
   },
   "source": [
    "**Regression**. In a regression setting, it is assumed that the observations satisfy for all $1\\leqslant i \\leqslant n$, $Y_i = f_\\star(X_i) + \\varepsilon_i$ where the $(\\varepsilon_i)_{1\\leqslant i\\leqslant n}$ are i.i.d. centered random variables in $\\mathbb{R}^M$, $X_i\\in\\mathbb{R}^d$ and $f_{\\star}$ is an unknown function.  The standard approach to estimate the parameters is by minimizing the mean square error:\n",
    "$$\n",
    "\\ell_n:\\theta \\mapsto  \\frac{1}{n} \\sum_{i=1}^n\\left\\|f_{\\theta}(X_i) - Y_i\\right\\|^2\\,,\n",
    "$$\n",
    "where $f_{\\theta}$ is a nonlinear parametric function used to estimate the unknown function $f_\\star$. In this session we focus on these **regression settings** to explain the backpropagation of gradients."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qJ0pEhvT4f53"
   },
   "source": [
    "## <font color=darkred> Part *I* : Feed Forward Neural Networks </font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "emO_nI5wNGIx"
   },
   "source": [
    "#### Preamble: linear models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "H1mBy-DtNGYQ"
   },
   "source": [
    "In a linear model, we propose to predict the observations using $f_\\theta : x \\mapsto x^\\top \\theta$.\n",
    "For instance, for scalar observations, this means that $Y_i$ is predicted by $X^\\top_i \\theta$.\n",
    "\n",
    "The penalized least squares estimate of $\\theta$ is defined as a solution to\n",
    "\n",
    "$$\n",
    "\\widehat \\theta\\in  \\mathrm{argmin}_{\\theta\\in\\mathbb{R}^d}\\,\\left( \\|Y - X\\theta\\|_2^2 + \\lambda \\|\\theta\\|_2^2\\right)\\,,\n",
    "$$\n",
    "\n",
    "where $\\lambda>0$, $Y = (Y_1,\\ldots, Y_n)^\\top$ and $X$ is the matrix whose $i$-th row is $X_i^\\top$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HPJF3lavO4__"
   },
   "source": [
    "The matrix $X^\\top X + \\lambda I_n$ is definite positive for all $\\lambda>0$ as for all $u\\in\\mathbb{R}^d$,\n",
    "\n",
    "$$\n",
    "u^\\top (X^\\top X + \\lambda I_n)u = \\|Xu\\|_2^2 + \\lambda \\|u\\|_2^2\\,,\n",
    "$$\n",
    "\n",
    "which is positive for all $u\\neq 0$.\n",
    "\n",
    "Therefore,  the matrix $X^TX + \\lambda I_n$ is invertible for all $\\lambda>0$. Using that for all $\\lambda>0$,\n",
    "\n",
    "$$\n",
    "\\nabla \\left(\\|Y - X\\theta\\|_2^2 + \\lambda \\|\\theta\\|_2^2\\right) = 2X^\\top X\\theta - 2X^\\top Y +  2\\lambda\\theta = 2\\left\\{\\left(X^\\top X + \\lambda I_d\\right)\\theta -X^\\top Y\\right\\}\\,.\n",
    "$$\n",
    "\n",
    "Therefore,\n",
    "\n",
    "$$\n",
    "\\widehat \\theta = \\left(X^\\top X + \\lambda I_d\\right)^{-1}X^\\top Y\\,.\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "WVIJlOGaPfRo"
   },
   "outputs": [],
   "source": [
    "import datetime\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.linear_model import RidgeCV\n",
    "from sklearn.model_selection import GridSearchCV, train_test_split\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.kernel_ridge import KernelRidge\n",
    "import pandas as pd\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "NOp2-UgzH8hB"
   },
   "outputs": [],
   "source": [
    "plt.rcParams.update(\n",
    "    {\n",
    "        \"font.size\": 25,\n",
    "        \"figure.figsize\": (14, 7),\n",
    "        \"axes.grid\": True,\n",
    "        \"grid.color\": \"#93a1a1\",\n",
    "        \"grid.alpha\": 0.3,\n",
    "        \"axes.spines.top\": False,\n",
    "        \"axes.spines.right\": False,\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=darkred>Write a function ``sample_data_iid_linear`` to sample observations from the following model.</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BcIbrPtJc5oz"
   },
   "source": [
    "**Observed data**: $Y_k = \\sum_{j=1}^d \\beta_j X_k(j) + \\varepsilon_k$, where $\\varepsilon_k$ are i.i.d. $\\mathcal{N}(0,\\sigma^2)$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_L8Vy8MtPf6-"
   },
   "outputs": [],
   "source": [
    "# function to sample a dataset - linear observation model\n",
    "def sample_data_iid_linear(beta, sigmay, n, d):\n",
    "    X = np.random.uniform(0, 2 * np.pi, [n,d])\n",
    "    Y = np.zeros(n)\n",
    "    eta = np.random.normal(loc=0, scale=1, size=n)\n",
    "    Y[0] = np.sum(beta * X[0,:]) + sigmay * eta[0]\n",
    "    for k in range(1,n):\n",
    "        Y[k] = np.sum(beta * X[k,:]) + sigmay * eta[k]\n",
    "    return X, Y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=darkred>Write a function ``sample_data_iid_nonlinear`` to sample observations from the following model.</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rj0w_VPzdNVN"
   },
   "source": [
    "**Observed data**: $Y_k = \\sum_{j=1}^d \\cos(X_k(j)) + \\varepsilon_k$, where $\\varepsilon_k$ are i.i.d. $\\mathcal{N}(0,\\sigma^2)$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Rwjhv6zZPlDW"
   },
   "outputs": [],
   "source": [
    "# function to sample a dataset - nonlinear observation model\n",
    "def sample_data_iid_nonlinear(sigmay, n, d):\n",
    "    X = np.random.uniform(0, 2 * np.pi, [n,d])\n",
    "    Y = np.zeros(n)\n",
    "    eta = np.random.normal(loc=0, scale=1, size=n)\n",
    "    Y[0] = np.sum(np.cos(X[0,:])) + sigmay * eta[0]\n",
    "    for k in range(1,n):\n",
    "        Y[k] = np.sum(np.cos(X[k,:])) + sigmay * eta[k]\n",
    "    return X, Y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=darkred>Sample a dataset (train and test) with the linear observation model</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7vpDY0u1PoMp"
   },
   "outputs": [],
   "source": [
    "# set parameters\n",
    "n_train = 1000\n",
    "n_test = 100\n",
    "d = 2\n",
    "sigmay = 0.5\n",
    "beta = np.random.multivariate_normal(np.zeros(d), np.eye(d))\n",
    "\n",
    "# sample data\n",
    "X_train, Y_train = sample_data_iid_linear(beta, sigmay, n_train, d)\n",
    "X_test, Y_test = sample_data_iid_linear(beta, sigmay, n_test, d)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=darkred>Train a Ridge regression model using RidgeCV from sklearn.linear_model</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "AlDWDWB_PoSn",
    "outputId": "f2bbe40b-6a3c-43fb-ce40-65046266895d"
   },
   "outputs": [],
   "source": [
    "linear_ridge_model = Ridge()\n",
    "parameters = {'alpha': [1e-5, 1e-4, 1e-3, 1e-2, 1e-1,1, 2, 3, 4, 5]}\n",
    "ridge_regressor = GridSearchCV(linear_ridge_model, parameters, scoring='neg_mean_squared_error', cv=5)\n",
    "ridge_regressor.fit(X_train, Y_train)\n",
    "ridge_regressor.best_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 425
    },
    "id": "lRhQ6zfyRDqu",
    "outputId": "99200671-f20c-4bf5-b71f-5d12f1b7e78e"
   },
   "outputs": [],
   "source": [
    "# plot results\n",
    "Y_pred_linear = ridge_regressor.predict(X_test)\n",
    "plt.plot(Y_test,linestyle = \"dashed\",color=\"blue\", label= \"True observations\")\n",
    "plt.plot(Y_pred_linear,linestyle = \"dashed\",color=\"red\", label=\"Predictions\")\n",
    "plt.ylabel('Observations', fontsize=12)\n",
    "plt.xlabel('Individual indices', fontsize=12)\n",
    "plt.tick_params(labelright=True)\n",
    "plt.grid('True')\n",
    "plt.legend();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 425
    },
    "id": "2gB-e1BgIP39",
    "outputId": "ffb6c6e1-90bc-45b8-82ff-5690f8dd745e"
   },
   "outputs": [],
   "source": [
    "# plot results\n",
    "Y_pred_linear = ridge_regressor.predict(X_test)\n",
    "# with respect to x_1\n",
    "sort_index = np.argsort(X_test[:,0])\n",
    "plt.plot(X_test[sort_index,0],Y_test[sort_index],linestyle = \"dashed\",color=\"black\", label= \"True observations wrt x_1\")\n",
    "plt.plot(X_test[sort_index,0],Y_pred_linear[sort_index],linestyle = \"dashed\",color=\"red\", label=\"Predictions wrt x_1\")\n",
    "# with respect to x_2\n",
    "#sort_index = np.argsort(X_test[:,1])\n",
    "#plt.plot(X_test[sort_index,1],Y_test[sort_index],linestyle = \"solid\",color=\"black\", label= \"True observations wrt x_2\")\n",
    "#plt.plot(X_test[sort_index,1],Y_pred_linear[sort_index],linestyle = \"solid\",color=\"red\", label=\"Predictions wrt x_2\")\n",
    "plt.ylabel('Observations', fontsize=12)\n",
    "plt.xlabel('Coordinate of X', fontsize=12)\n",
    "plt.tick_params(labelright=True)\n",
    "plt.grid('True')\n",
    "plt.legend();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=darkred>Perform the analysis again with data sampled from the nonlinear observation model ``sample_data_iid_nonlinear``</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 447
    },
    "id": "H-73XoWCVDaf",
    "outputId": "18e864f5-8aac-4c49-eb21-54eccc1264ab"
   },
   "outputs": [],
   "source": [
    "# sample data\n",
    "X_train, Y_train = sample_data_iid_nonlinear(sigmay, n_train, d)\n",
    "X_test, Y_test = sample_data_iid_nonlinear(sigmay, n_test, d)\n",
    "\n",
    "ridge_regressor.fit(X_train, Y_train)\n",
    "ridge_regressor.best_score_\n",
    "\n",
    "# plot results\n",
    "Y_pred_linear = ridge_regressor.predict(X_test)\n",
    "# with respect to x_1\n",
    "sort_index = np.argsort(X_test[:,0])\n",
    "plt.plot(X_test[sort_index,0],Y_test[sort_index],linestyle = \"dashed\",color=\"black\", label= \"True observations wrt x_1\")\n",
    "plt.plot(X_test[sort_index,0],Y_pred_linear[sort_index],linestyle = \"dashed\",color=\"red\", label=\"Predictions wrt x_1\")\n",
    "# with respect to x_2\n",
    "#sort_index = np.argsort(X_test[:,1])\n",
    "#plt.plot(X_test[sort_index,1],Y_test[sort_index],linestyle = \"solid\",color=\"black\", label= \"True observations wrt x_2\")\n",
    "#plt.plot(X_test[sort_index,1],Y_pred_linear[sort_index],linestyle = \"solid\",color=\"red\", label=\"Predictions wrt x_2\")\n",
    "plt.ylabel('Observations', fontsize=12)\n",
    "plt.xlabel('Coordinate of X', fontsize=12)\n",
    "plt.tick_params(labelright=True)\n",
    "plt.grid('True')\n",
    "plt.legend();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "axHyOnJDRNd2"
   },
   "source": [
    "In this case, the linear model **has very poor performance**. This is no surprise as the observations were generated using a nonlinear transform of the input.\n",
    "\n",
    "This session is devoted to more complex nonlinear models to transform input data based of **successive combinations of linear and nonlinear transformation of the input**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IfGOH3e962GX"
   },
   "source": [
    "#### Multi-layer models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2gbBjgAZ652x"
   },
   "source": [
    "The **Multi-layer Perceptron (MLP)** or **Feed Forward Neural Network (FFNN)** weakens the modeling assumptions of the logistic regression and composed in parallel  $L$ perceptron units to produce the output. Let $x\\in\\mathbb{R}^d$ be the input and define all layers as follows.\n",
    "\n",
    "\\begin{align*}\n",
    "h_{\\theta}^0(x) &= x\\,,\\\\\n",
    "\\mathrm{Linear\\; transform}\\qquad z_{\\theta}^k(x)  &= b^k + W^kh_{\\theta}^{k-1}(x)\\quad \\mathrm{for\\;all\\;} 1\\leqslant k\\leqslant L\\,,\\\\\n",
    "\\mathrm{Nonlinear\\; transform}\\qquad h_{\\theta}^k(x)  &= \\varphi_k(z_{\\theta}^{k}(x))\\quad \\mathrm{for\\;all\\;}1\\leqslant k\\leqslant L\\,,\n",
    "\\end{align*}\n",
    "\n",
    "where for all $1 \\leqslant k \\leqslant L$, $b^k\\in\\mathbb{R}^{d_k}$, $W^k\\in\\mathbb{R}^{d_k\\times d_{k-1}}$, with $d_0 \\equiv d$, and $\\varphi^k: \\mathbb{R}^{d_k} \\to \\mathbb{R}^{d_k}$ is a nonlinear activation function.\n",
    "\n",
    "Let **$\\theta = \\{b^1,W^1,\\ldots,b^L,W^L\\}$ be the unknown parameters of the MLP and $f_{\\theta}(x) = h_{\\theta}^L(x)$** be the output layer of the MLP."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jMfNVlY66z81"
   },
   "source": [
    "<font color=darkred>**Activation functions**</font>. As there is no modelling assumptions anymore, virtually any activation functions $\\varphi^k$, $1 \\leqslant k \\leqslant L$ may be used. In this section, it is assumed that these intermediate activation functions apply elementwise and, with a minor abuse of notations, we write for all $1 \\leqslant k \\leqslant L$ and all $z\\in\\mathbb{R}^{d_m}$,\n",
    "$$\n",
    "\\varphi^k(z) = (\\varphi^k(z_1),\\ldots, \\varphi^k(z_{d_m}))\\,,\n",
    "$$\n",
    "with $\\varphi^k: \\mathbb{R}\\to \\mathbb{R}$ the seleced scalar activation function. The rectified linear unit (RELU) activation function $x \\mapsto \\mathrm{max}(0,x)$ and its extensions are the default recommendation in modern implementations  ``(Jarrettet al., 2009; Nair and Hinton, 2010; Glorot et al., 2011a), (Maas et al.,2013),  (He et al., 2015)``. One of the major motivations arise from the gradient based parameter optimization which is numerically more stable with this choice. The choice of the last activation function $\\varphi^L$ greatly relies on the task the network is assumed to perform.\n",
    "\n",
    "- **Biclass classification**. The output $h_{\\theta}^L(x)$ is the estimate of the probability that the class is $1$ given the input $x$. The common choice in this case is the sigmoid function. Then, $d_L = 1$ and $h_{\\theta}^L(x)$ contains $\\mathbb{P}(Y=1 | X)$ and is enough to use as a plug-in Bayes classifier.\n",
    "$$\n",
    "\\varphi^L(z) =  \\frac{\\mathrm{e}^{z}}{1 + \\mathrm{e}^{z}}\\,.\n",
    "$$\n",
    "- **Multiclass classification**. The output $h_{\\theta}^L(x)$ is the estimate of the probability that the class is  $k$ for all $1\\leqslant k\\leqslant M$, given the input $x$. The common choice in this case is the softmax function: for all $1\\leqslant i\\leqslant M$\n",
    "$$\n",
    "\\varphi^L(z)_i = \\mathrm{softmax}(z)_i = \\frac{\\mathrm{e}^{z_i}}{\\sum_{j=1}^M\\mathrm{e}^{z_j}}\\,.\n",
    "$$\n",
    "In this case $d_L = M$ and each component $k$ of $h_{\\theta}^L(x)$ contains $\\mathbb{P}(Y=k | X)$.\n",
    "- **Regression**. The output $h_{\\theta}^L(x)$ is $f_\\theta(x)$. A common choice for the last activation function is the *identity function* i.e, the last transform is only a linear transform."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fDPT2XKZ_lzn"
   },
   "source": [
    "<font color=darkred>**Training of $\\theta$**</font>. For our regression task, the standard approach to estimate the parameters is by maximizing the logarithm of the likelihood i.e. by minimizing the opposite of the normalized loglikelihood:\n",
    "$$\n",
    "\\ell_n:\\theta \\mapsto -\\frac{1}{n} \\sum_{i=1}^n \\|Y_i - f_{\\theta}(X_i)\\|^2\\,.\n",
    "$$\n",
    "\n",
    "To estimate the unknown parameter $\\theta$, we use gradient descent algorithm which amounts to producing recursively parameter estimates using an update of the form:\n",
    "$$\\theta^{(k+1)} = \\theta^{(k)} - \\eta_{k+1} \\nabla \\ell_n(\\theta^{(k)})\\,.$$\n",
    "Therefore, it is **required to compute $\\nabla \\ell_n(\\theta^{(k)})$ efficiently**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SIBL36SPtjnk"
   },
   "source": [
    "#### Historical derivation of activation functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uFIDVr5Ktjyv"
   },
   "source": [
    "The first  mathematical model for a neuron was the **Threshold Logic Unit** ``(McCulloch and Pitts, 1943)``, with Boolean inputs and outputs. In this setting, the (deterministic) response associated with an input $x\\in\\{0,1\\}^d$ is defined as\n",
    "$$\n",
    "g: x\\mapsto \\mathbb{1}_{\\omega\\sum_{j=1}^dx_j + b \\geqslant 0}\\,.\n",
    "$$\n",
    "This elementary model can be extended to real valued inputs ``(Rosenblatt, 1957)`` with\n",
    "$$\n",
    "g: x\\mapsto \\mathbb{1}_{\\sum_{j=1}^d\\omega_jx_j + b \\geqslant 0}\\,.\n",
    "$$\n",
    "In this case, the nonlinear activation function is $\\sigma: x \\mapsto \\mathbb{1}_{x\\geqslant 0}$ and the ouput in $\\{0,1\\}$ defined as:\n",
    "$$\n",
    "g:x \\mapsto \\sigma(\\omega^Tx + b)\\,.\n",
    "$$\n",
    "\n",
    "**Logistic regression** is another instance with the sigmoid **activation function** $\\sigma: x\\mapsto \\mathrm{e}^x/(1+\\mathrm{e}^x)$ and the output $ \\sigma(\\omega^TX + b)$ in $(0,1)$ is $\\mathbb{P}(Y=1 | X)$.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zVzthzG1tj44"
   },
   "source": [
    "#### Implementation with Keras - Dataset \"water management\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://www.kaggle.com/sudalairajkumar/chennai-water-management/version/3\n",
    "This dataset has details about the water availability in the four reservoirs over the last 15 years. We use the information about 3 reservoirs to predict the information about the last one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "# Load data\n",
    "dataset = np.loadtxt('./chennai_reservoir_levels.csv', delimiter='|', skiprows=1, usecols=(1,2,3,4))\n",
    "\n",
    "# Shuffle dataset\n",
    "np.random.shuffle(dataset)\n",
    "\n",
    "# Separate features and targets\n",
    "X = dataset[:, 0:3]\n",
    "y = dataset[:, 3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train: (4235, 3)\n",
      "y_train: (4235,)\n",
      "X_test: (1412, 3)\n",
      "y_test: (1412,)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y)\n",
    "print(f\"X_train: {X_train.shape}\\ny_train: {y_train.shape}\")\n",
    "print(f\"X_test: {X_test.shape}\\ny_test: {y_test.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=darkred>Scale data using ``StandardScaler`` from ``sklearn.preprocessing``</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "std_scaler = StandardScaler()\n",
    "X_train_sc = std_scaler.fit_transform(X_train)\n",
    "X_test_sc = std_scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "whaMHTp3P0oi"
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0sroA9-7KjR_"
   },
   "source": [
    "**Model design**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_d = 3\n",
    "hidden_d = 8"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=darkred>Build one feed forward neural network with one hidden layer with dimension ``hidden_d`` with activation function ``relu``</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(Dense(input_d, input_shape=(11,), activation='relu'))\n",
    "model.add(Dense(hidden_d, activation='relu'))\n",
    "model.add(Dense(1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=darkred>Set the optimizer as ``adam`` and the loss function as the ``mean_squared_error``</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "FGl08JbknXaT",
    "outputId": "a8b3a7a7-e0f4-4fda-eb9d-ca69731f090c"
   },
   "outputs": [],
   "source": [
    "# Chose loss and optimizer for the training\n",
    "model.compile(loss='mean_squared_error', optimizer='adam')\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_P-9MLEeKlly"
   },
   "source": [
    "**Model training**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=darkred>Train the model using ``model.fit``</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 615
    },
    "id": "KzA7HWHnn2Kz",
    "outputId": "06bfe9b2-65a5-48f8-f7e2-f09ad3580e7f"
   },
   "outputs": [],
   "source": [
    "# Fit the model\n",
    "history = model.fit(X, y, epochs=5, batch_size=5, verbose=1, shuffle=True)\n",
    "# Plot loss history\n",
    "plt.xlabel(\"Epochs\")\n",
    "plt.ylabel(\"MSE\")\n",
    "plt.plot(history.history[\"loss\"]);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=darkred>Display predictions on the train dataset</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 444
    },
    "id": "jUk8yV1swi8P",
    "outputId": "6e89a7d3-5d1f-4c33-a069-0eac5c4a9408"
   },
   "outputs": [],
   "source": [
    "y_pred_train = model.predict(X_train_sc).squeeze()\n",
    "sns.stripplot(data=y_train, alpha=.4, label=\"ground truth\")\n",
    "sns.stripplot(data=y_pred_train, alpha=.4, label=\"predictions\")\n",
    "plt.ylabel(\"Water level\")\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=darkred>Display predictions on the test dataset</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test_train = model.predict(X_test_sc).squeeze()\n",
    "sns.stripplot(data=y_test, alpha=.4, label=\"ground truth\")\n",
    "sns.stripplot(data=y_pred_test, alpha=.4, label=\"predictions\")\n",
    "plt.ylabel(\"Water level\")\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=darkred>Analyze the impact of the number of layers.</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nmtgv6WzsrAm"
   },
   "source": [
    "## <font color=darkred> Part *II* : Backpropagation of gradients from scratch </font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "k8cYSs75Ch9K"
   },
   "source": [
    "#### Rationale"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1BVZMM2p1YoD"
   },
   "source": [
    "The key ingredient to compute gradients is the **chain rule**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AYGV2zEKdp92"
   },
   "source": [
    "**Forward pass**: Starting from the input, ans the current value of the parameter, we can compute and store all linear transforms $z_\\theta^k(x)$ and nonlinear transforms $h_\\theta^k(x)$.\n",
    "\n",
    "\n",
    "**Backward pass**: Starting from the output $f_\\theta(x) = h_\\theta^L(x)$, we compute the gradient of the loss function by recursively applying the chain rule, using the quantities computed in the forward pass.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "b9IBZ_quCiYt"
   },
   "source": [
    "#### Complete derivation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eIDcGpBb7d_0"
   },
   "source": [
    "Assume here that the last activation function is the identity function. The output is then $h_{\\theta}^L(x)= f_{\\theta}(x) $ so that it is the prediction of the observation associated with the input $x$.\n",
    "\n",
    "Write\n",
    "$$\n",
    "\\ell_n:\\theta \\mapsto \\frac{1}{n} \\sum_{i=1}^n \\ell_{\\theta}(X_i,Y_i) = \\frac{1}{n} \\sum_{i=1}^n \\|Y_i-f_\\theta(X_i)\\|^2\\,.\n",
    "$$\n",
    "\n",
    "Therefore, **the gradient with respect to all parameters can be computed recursively as follows**.\n",
    "\n",
    "\n",
    "Then, for all $1\\leqslant m\\leqslant L-1$,\n",
    "\\begin{align*}\n",
    "\\nabla_{W^m} \\ell_{\\theta}(X,Y) &= \\nabla_{z_{\\theta}^m(X)}\\ell_{\\theta}(X,Y)(h_{\\theta}^{m-1}(X))^\\top\\,,\\\\\n",
    "\\nabla_{b^m} \\ell_{\\theta}(X,Y) &=  \\nabla_{z_{\\theta}^m(X)}\\ell_{\\theta}(X,Y)\\,,\n",
    "\\end{align*}\n",
    "where $\\nabla_{z_{\\theta}^m(X)}$ is computed recursively as follows.\n",
    "\\begin{align*}\n",
    "\\nabla_{h_{\\theta}^m(X)} \\ell_{\\theta}(X,Y) &= (W^{m+1})^\\top\\nabla_{z_{\\theta}^{m+1}(X)} \\ell_{\\theta}(X,Y) \\,,\\\\\n",
    "\\nabla_{z_{\\theta}^m(X)} \\ell_{\\theta}(X,Y) &= \\nabla_{h_{\\theta}^m(X)}\\ell_{\\theta}(X,Y) \\odot \\varphi_m'(z_{\\theta}^{m}(X))\\,,\n",
    "\\end{align*}\n",
    "where $\\odot$ is the elementwise multiplication.\n",
    "\n",
    "All gradients can be computed in two steps.\n",
    "- In a ``forward pass``, the quantities $z_{\\theta}^{m}(X)$ and $h_{\\theta}^{m}(X)$ can be computed for all $0\\leq m \\leq M$.\n",
    "\n",
    "- In a ``backward pass``, the gradients can be computed recursively, using these quantities and the update formula.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2v0zURGNFmZe"
   },
   "source": [
    "#### Proof"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fjcu42mEFpjx"
   },
   "source": [
    "To obtain the recursive formulation of the gradient computations, write, for all $1\\leqslant m \\leqslant L-1$ and all $1\\leqslant j \\leqslant d_m$,  using that $z_{\\theta}^{m+1}(X) = b^{m+1} + W^{m+1}h_{\\theta}^{m}(X)$,\n",
    "\\begin{align*}\n",
    "\\partial_{(h_{\\theta}^m(X))_j}\\ell_{\\theta}(X,Y) &=  \\sum_{i=1}^{d_{m+1}} \\partial_{(z_{\\theta}^{m+1}(X))_i}\\ell_{\\theta}(X,Y)\\partial_{(h_{\\theta}^m(X))_j}(z_{\\theta}^{m+1}(X))_i\\,,\\\\\n",
    "&=  \\sum_{i=1}^{d_{m+1}} \\partial_{(z_{\\theta}^{m+1}(X))_i}\\ell_{\\theta}(X,Y)W^{m+1}_{i,j}\\,.\n",
    "\\end{align*}\n",
    "Therefore,\n",
    "$$\n",
    "\\nabla_{h_{\\theta}^m(X)} \\ell_{\\theta}(X,Y) = (W^{m+1})^\\top\\nabla_{z_{\\theta}^{m+1}(X)} \\ell_{\\theta}(X,Y) \\,.\n",
    "$$\n",
    "Then, for all $1\\leqslant m \\leqslant L-1$ and all $1\\leqslant j \\leqslant d_m$, ,  using that $h_{\\theta}^{m}(X)_j =\\varphi_m(z_{\\theta}^{m}(X)_j)$,\n",
    "\\begin{align*}\n",
    "\\partial_{(z_{\\theta}^m(X))_j}\\ell_{\\theta}(X,Y) &=  \\sum_{i=1}^{d_{m+1}} \\partial_{(h_{\\theta}^m(X))_i}\\ell_{\\theta}(X,Y)\\partial_{(z_{\\theta}^m(X))_j}(h_{\\theta}^m(X))_i\\,,\\\\\n",
    "&=  \\sum_{i=1}^{d_{m+1}} \\partial_{(h_{\\theta}^m(X))_i}\\ell_{\\theta}(X,Y) \\mathbb{1}_{i=j}\\varphi_m'(z_{\\theta}^{m}(X)_i)\\,,\\\\\n",
    "&= \\partial_{(h_{\\theta}^m(X))_j}\\ell_{\\theta}(X,Y) \\varphi_m'(z_{\\theta}^{m}(X)_j)\\,.\n",
    "\\end{align*}\n",
    "Therefore,\n",
    "$$\n",
    "\\nabla_{z_{\\theta}^m(X)} \\ell_{\\theta}(X,Y) = \\nabla_{h_{\\theta}^m(X)}\\ell_{\\theta}(X,Y) \\odot \\varphi_m'(z_{\\theta}^{m}(X))\\,.\n",
    "$$\n",
    "Then, for all $1\\leqslant i\\leqslant d_m$ and all $1\\leqslant j \\leqslant d_{m-1}$, and using that $z_{\\theta}^m(X) = b^m + W^mh_{\\theta}^{m-1}(X)$,\n",
    "\\begin{align*}\n",
    "\\partial_{W^m_{i,j}}\\ell_{\\theta}(X,Y) &=  \\sum_{k=1}^{d_m} \\partial_{(z_{\\theta}^m(X))_k}\\ell_{\\theta}(X,Y)\\partial_{W^m_{i,j}}(z_{\\theta}^m(X))_k\\,,\\\\\n",
    "&=  \\sum_{k=1}^{d_m} \\partial_{(z_{\\theta}^m(X))_k}\\ell_{\\theta}(X,Y)\\mathbb{1}_{i=k}(h_{\\theta}^{m-1}(X))_j\\,,\\\\\n",
    "&=  \\partial_{(z_{\\theta}^m(X))_i}\\ell_{\\theta}(X,Y)(h_{\\theta}^{m-1}(X))_j\\,.\n",
    "\\end{align*}\n",
    "Therefore,\n",
    "$$\n",
    "\\nabla_{W^m} \\ell_{\\theta}(X,Y) = \\nabla_{z_{\\theta}^m(X)}\\ell_{\\theta}(X,Y)(h_{\\theta}^{m-1}(X))^\\top\\,.\n",
    "$$\n",
    "Similarly, for all $1\\leqslant i\\leqslant d_m$,  using that $z_{\\theta}^m(X) = b^m + W^mh_{\\theta}^{m-1}(X)$,\n",
    "\\begin{align*}\n",
    "\\partial_{b^m_{i}}\\ell_{\\theta}(X,Y) &=  \\sum_{k=1}^{d_m} \\partial_{(z_{\\theta}^m(X))_k}\\ell_{\\theta}(X,Y)\\partial_{b^m_{i}}(z_{\\theta}^m(X))_k\\,,\\\\\n",
    "&=  \\sum_{k=1}^{d_m} \\partial_{(z_{\\theta}^m(X))_k}\\ell_{\\theta}(X,Y)\\mathbb{1}_{i=k},\\\\\n",
    "&=  \\partial_{(z_{\\theta}^m(X))_k}\\ell_{\\theta}(X,Y)_i\\,.\n",
    "\\end{align*}\n",
    "Therefore,\n",
    "$$\n",
    "\\nabla_{b^m} \\ell_{\\theta}(X,Y) =  \\nabla_{z_{\\theta}^m(X)}\\ell_{\\theta}(X,Y)\\,.\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_Swr_xryusve"
   },
   "source": [
    "#### Implementation from scratch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "trrYcNBGXwOY"
   },
   "outputs": [],
   "source": [
    "# sigmoid and its derivative\n",
    "def sigmoid(X):\n",
    "    return 1 / (1 + np.exp(-X))\n",
    "\n",
    "\n",
    "def dsigmoid(X):\n",
    "    sig=sigmoid(X)\n",
    "    return sig * (1 - sig)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CIpmTE-JYKwq"
   },
   "source": [
    "We implement a simple neural network with one hidden layer:\n",
    "\\begin{align*}\n",
    "h_\\theta^1(x) &= \\sigma(W^hx+b^h)\\,,\\\\\n",
    "f_\\theta(x) &= \\sigma(W^o h_\\theta^1(x)+b^o)\\,,\\\\\n",
    "\\end{align*}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=darkred>Build a class ``NeuralNet`` implementing the forward and backward passes of this  feed forward neural network</font>\n",
    "\n",
    "<font color=darkred>   - A method ``forward`` performing the forward pass with input ``X`` and oputput ``y``</font>\n",
    "\n",
    "<font color=darkred>   - A method ``grad_loss`` computing the gradient of the loss function with respect to $W^h$, $W^o$, $b^h$, $b^o$.</font>\n",
    "\n",
    "<font color=darkred>   - A method ``train`` updating $W^h$, $W^o$, $b^h$, $b^o$ with one traditional SGD step.</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "JW7Yc151XwQu"
   },
   "outputs": [],
   "source": [
    "class NeuralNet():\n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        self.W_h = np.random.uniform(\n",
    "            size=(input_size, hidden_size), high=0.01, low=-0.01)\n",
    "        self.b_h = np.zeros(hidden_size)\n",
    "        self.W_o = np.random.uniform(\n",
    "            size=(hidden_size, output_size), high=0.01, low=-0.01)\n",
    "        self.b_o = np.zeros(output_size)\n",
    "        self.output_size = output_size\n",
    "\n",
    "    def forward(self, X, keep_activations=False):\n",
    "        z_h = np.dot(X, self.W_h) + self.b_h\n",
    "        h = sigmoid(z_h)\n",
    "        z_o = np.dot(h, self.W_o) + self.b_o\n",
    "        y = softmax(z_o)\n",
    "        \n",
    "        if keep_activations:\n",
    "            return y, h, z_h\n",
    "        else:\n",
    "            return y\n",
    "\n",
    "    def loss(self, X, y):\n",
    "        return nll(one_hot(self.output_size, y), self.forward(X))\n",
    "\n",
    "    def grad_loss(self, x, y_true):\n",
    "        y, h, z_h = self.forward(x, keep_activations=True)\n",
    "        grad_z_o = y - one_hot(self.output_size, y_true)\n",
    "\n",
    "        grad_W_o = np.outer(h, grad_z_o)\n",
    "        grad_b_o = grad_z_o\n",
    "        grad_h = np.dot(grad_z_o, np.transpose(self.W_o))\n",
    "        grad_z_h = grad_h * dsigmoid(z_h)\n",
    "        grad_W_h = np.outer(x, grad_z_h)\n",
    "        grad_b_h = grad_z_h\n",
    "        grads = {\"W_h\": grad_W_h, \"b_h\": grad_b_h,\n",
    "                 \"W_o\": grad_W_o, \"b_o\": grad_b_o}\n",
    "        return grads\n",
    "\n",
    "    def train(self, x, y, learning_rate):\n",
    "        # Traditional SGD update on one sample at a time\n",
    "        grads = self.grad_loss(x, y)\n",
    "        self.W_h = self.W_h - learning_rate * grads[\"W_h\"]\n",
    "        self.b_h = self.b_h - learning_rate * grads[\"b_h\"]\n",
    "        self.W_o = self.W_o - learning_rate * grads[\"W_o\"]\n",
    "        self.b_o = self.b_o - learning_rate * grads[\"b_o\"]\n",
    "\n",
    "    def predict(self, X):\n",
    "        if len(X.shape) == 1:\n",
    "            return np.argmax(self.forward(X))\n",
    "        else:\n",
    "            return np.argmax(self.forward(X), axis=1)\n",
    "\n",
    "    def accuracy(self, X, y):\n",
    "        y_preds = np.argmax(self.forward(X), axis=1)\n",
    "        return np.mean(y_preds == y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [
    "2v0zURGNFmZe"
   ],
   "gpuType": "T4",
   "provenance": [],
   "toc_visible": true
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
